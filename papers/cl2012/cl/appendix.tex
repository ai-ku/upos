%% \section{Co-occurrence Modeling}
%% \label{sec:code}

\appendix
%%\appendixsection{Algorithm}
%%\label{sec:algorithm}
%% [!! remove this part]
%% In this section, we briefly describe the components of our algorithm.
%% Section~\ref{sec:subcomp} presents the motive and the method for
%% computation of substitute vectors, our paradigmatic representations
%% for the word contexts.  We combine the substitute vectors with the
%% identity and features of the target word for part of speech induction
%% using the a co-occurrence modeling framework.

\appendixsection{Computation of Substitute Distributions}
\label{sec:subcomp}

In this study, we predict the syntactic category of a word in a given
context based on its substitute distribution.  The sample space of the
substitute distribution is the vocabulary of the language model
including the unknow word tag ``\_unk\_''.  Note that the substitute
distribution is a function of the context only and is indifferent to
the target word.

%% The dimensions of the
%% substitute distribution represent words in the vocabulary of the
%% language model, and the entries in the substitute distribution
%% represent the probability of those words being used in the given
%% context.  

% how are the substitutes computed
It is best to use both the left and the right context when estimating
the probabilities for potential lexical substitutes.  For example, in
\emph{``He lived in San Francisco suburbs.''}, the token \emph{San}
would be difficult to guess from the left context but it is almost
certain looking at the right context.  We define $c_w$ as the $2n-1$
word window centered around the target word position: $w_{-n+1} \ldots
w_0 \ldots w_{n-1}$ ($n=4$ is the n-gram order).  The probability of a
substitute word $w$ in a given context $c_w$ can be estimated as:
\begin{eqnarray}
  \label{eq:lm1}P(w_0 = w | c_w) & \propto & P(w_{-n+1}\ldots w_0\ldots w_{n-1})\\
  \label{eq:lm2}& = & P(w_{-n+1})P(w_{-n+2}|w_{-n+1})\nonumber\\
  &&\ldots P(w_{n-1}|w_{-n+1}^{n-2})\\
  \label{eq:lm3}& \approx & P(w_0| w_{-n+1}^{-1})P(w_{1}|w_{-n+2}^0)\nonumber\\
  &&\ldots P(w_{n-1}|w_0^{n-2})
\end{eqnarray}
where $w_i^j$ represents the sequence of words $w_i w_{i+1} \ldots
w_{j}$.  In Equation \ref{eq:lm1}, $P(w|c_w)$ is proportional to
$P(w_{-n+1}\ldots w_0 \ldots w_{n-1})$ because the words of the
context are fixed.  Terms without $w_0$ are identical for each
substitute in Equation \ref{eq:lm2} therefore they have been dropped
in Equation \ref{eq:lm3}.  Finally, because of the Markov property of
n-gram language model, only the closest $n-1$ words are used in the
experiments.

Near the sentence boundaries the appropriate terms were truncated in
Equation \ref{eq:lm3}.  Specifically, at the beginning of the sentence
shorter n-gram contexts were used and at the end of the sentence terms
beyond the end-of-sentence token were dropped.

To obtain a discrete representation of the context, the
random-substitutes algorithm pairs each word token with a substitute
sampled from the pre-computed substitute distribution generated from
the word token's context and then word ($W$) -- random-substitute
($S$) pairs are fed to the S-CODE as input.

\appendixsection{The CODE Model}
\label{sec:codethr}

In this section we review the unsupervised method that we use to model
co-occurrence statistics: the Co-occurrence Data Embedding
(CODE)\ \cite{globerson2007euclidean} method and its spherical
extension (S-CODE) introduced by \cite{maron2010sphere}.

Let $W$ and $S$ be two categorical variables with finite cardinalities
$|W|$ and $|S|$.  We observe a set of pairs $\{w_i, s_i\}_{i=1}^n$
drawn IID from the joint distribution of $W$ and $S$.  The basic idea
behind CODE and related methods is to represent (embed) each value of
$W$ and each value of $S$ as points in a common Euclidean space
$\mathbf{R}^d$ such that values that frequently co-occur lie close to
each other.  There are several ways to formalize the relationship
between the distances and co-occurrence statistics, in this paper we
use the following:
\begin{equation} \label{eq:probability}
p(w,s) = \frac{1}{Z} \bar{p}(w) \bar{p}(s) e^{-d^2_{w,s}}
\end{equation}
\noindent where $d^2_{w,s}$ is the squared distance between the
embeddings of $w$ and $s$, $\bar{p}(w)$ and $\bar{p}(s)$ are empirical
probabilities, and $Z=\sum_{w,s} \bar{p}(w) \bar{p}(w) e^{-d^2_{w,s}}$
is a normalization term.  If we use the notation $\phi_w$ for the
point corresponding to $w$ and $\psi_s$ for the point corresponding to
$s$ then $d^2_{w,s} = \|\phi_w-\psi_s\|^2$.  The log-likelihood of a
given embedding $\ell(\phi, \psi)$ can be expressed as:
\begin{eqnarray}
&&\ell(\phi, \psi) = \sum_{w,s} \bar{p}(w,s) \log p(w,s) \label{eq:likelihood} \\
&&= \sum_{w,s} \bar{p}(w,s) (-\log Z + \log \bar{p}(w)\bar{p}(s) - d^2_{w,s}) \nonumber \\
&&= -\log Z + \mathit{const} - \sum_{w,s} \bar{p}(w,s) d^2_{w,s} \nonumber
\end{eqnarray}
The likelihood is not convex in $\phi$ and $\psi$.  We use gradient
ascent to find an approximate solution for a set of $\phi_w$, $\psi_s$
that maximize the likelihood.  The gradient of the $d^2_{w,s}$ term
pulls neighbors closer in proportion to the empirical joint
probability:
\begin{equation}
\frac{\partial}{\partial\phi_w} \sum_{w,s} -\bar{p}(w,s) d^2_{w,s} =
\sum_y 2 \bar{p}(w,s) (\psi_s - \phi_w) \label{eq:attract}
\end{equation}
The gradient of the $Z$ term pushes neighbors apart in proportion to the
estimated joint probability:
\begin{equation}
\frac{\partial}{\partial\phi_x} (-\log Z) = \sum_y 2 p(w,s) (\phi_w -
\psi_s) \label{eq:repulse}
\end{equation}
Thus the net effect is to pull pairs together if their estimated
probability is less than the empirical probability and to push them
apart otherwise.  The gradients with respect to $\psi_s$ are similar.

S-CODE \cite{maron2010sphere} additionally restricts all $\phi_w$ and
$\psi_s$ to lie on the unit sphere.  With this restriction, $Z$ stays
around a fixed value during gradient ascent.  This allows S-CODE to
substitute an approximate constant $\tilde{Z}$ in gradient
calculations for the real $Z$ for computational efficiency.  In our
experiments, we used S-CODE with its sampling based stochastic
gradient ascent algorithm and smoothly decreasing learning rate.

%% \subsection{S-Code with More than Two Variables}

%% In order to accommodate multiple feature types the CODE model needs to
%% be extended to handle more than two variables.
%% \cite{globerson2007euclidean} suggest the following likelihood
%% function:

%% \begin{eqnarray}
%% &\ell(\phi,& \psi^{(1)}, \ldots, \psi^{(K)}) = \label{eq:multicode}\\
%% &&\sum_k w_k \sum_{x,y^{(k)}} \bar{p}(x,y^{(k)}) \log p(x,y^{(k)}) \nonumber
%% \end{eqnarray}

%% \noindent where $Y^{(1)}, \ldots, Y^{(K)}$ are $K$ different variables
%% whose empirical joint distributions with $X$,
%% $\bar{p}(x,y^{(1)})\ldots\bar{p}(x,y^{(K)})$, are known.
%% Eq.~\ref{eq:multicode} then represents a set of CODE models
%% $p(x,y^{(k)})$ where each $Y^{(k)}$ has an embedding $\psi_y^{(k)}$
%% but all models share the same $\phi_x$ embedding.  The weights $w_k$
%% reflect the relative importance of each $Y^{(k)}$.

%% We adopt this likelihood function, set all $w_k=1$, let $X$
%% represent a word, $Y^{(1)}$ represent a random substitute and
%% $Y^{(2)}, \ldots, Y^{(K)}$ stand for various morphological and
%% orthographic features of the word.  With this setup, the training
%% procedure needs to change little: each time a word --
%% random-substitute pair is sampled, the relevant word -- feature pairs
%% are also generated and input to the gradient ascent algorithm.

%% S-CODE handles two variables, whereas underlying syntactic categories
%% can be captured by more than two different variables such as
%% contextual, morphologic and ortographic features.  S-CODE can be
%% extented to handle more than two variables in a way similar to the
%% multi variable extension of CODE \cite{globerson2007euclidean} with
%% the unit sphere restriction.  The log-likelihood at
%% Equation~\ref{eq:likelihood} can be redefined for $n+1$ different
%% categorical variables $X$, $Y_i$, $\hdots$ and $Y_n$ with finite
%% cardinalities $|X|$, $|Y_1|$, $\hdots$ and $|Y_n|$, respectively, as:
%% \begin{eqnarray}
%% &&\ell(\phi, \psi_1,\hdots,\psi_n) = \sum_{i=1}^n\sum_{x,y_i} \bar{p}(x,y_i) \log p(x,y_i) \label{eq:multiscode} \\
%% &&= \sum_{i=1}^n\sum_{x,y_i} \bar{p}(x,y_i) (-\log Z_i + \log \bar{p}(x)\bar{p}(y_i) - d^2_{x,y_i}) \nonumber \\
%% &&=-\sum_{i=1}^n(\log Z_i + \mathit{const}_i - \sum_{x,y_i} \bar{p}(x,y_i) d^2_{x,y_i}) \nonumber
%% \end{eqnarray}
%% where $\psi_i$ is the embedding of $y_i \in Y_i$ and $Z_i$ is the
%% normalization term of $p(x,y_i)$.  Thus the model is able to jointly
%% learn the embeddings when the pairwise co-occurence statistics,
%% $\bar{p}(x,y_i)$, are available for all $i$.

%% One problem with these setting is, not every $(x,y_i)$ pair is
%% observed in the data.  For example, the stem word ``\textbf{car}''
%% doesn't have any morphological feature, thus its morphological feature
%% is represented by a null value, ``X''.  However setting the unobserved
%% features to ``X'' leads to pulling the words with unobserved features
%% together even they are from different clusters or pushing the ones
%% with observed features apart even they are from same clusters.  To
%% solve this, during the gradient search we don't perform any pull or
%% push updates on embeddings if the value of $y_i$ is set to null.
