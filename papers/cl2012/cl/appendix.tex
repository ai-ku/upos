%% \section{Co-occurrence Modeling}
%% \label{sec:code}

\appendix
%%\appendixsection{Algorithm}
%%\label{sec:algorithm}
%% [!! remove this part]
%% In this section, we briefly describe the components of our algorithm.
%% Section~\ref{sec:subcomp} presents the motive and the method for
%% computation of substitute vectors, our paradigmatic representations
%% for the word contexts.  We combine the substitute vectors with the
%% identity and features of the target word for part of speech induction
%% using the a co-occurrence modeling framework.

\appendixsection{Computation of Substitute Distributions}
\label{app:subcomp}

In this study, we predict the syntactic category of a word in a given
context based on its substitute distribution.  The sample space of the
substitute distribution is the vocabulary of the language model
including the unknown word tag ``<unk>''.  Note that the substitute
distribution is a function of the context only and is indifferent to
the target word.

%% The dimensions of the
%% substitute distribution represent words in the vocabulary of the
%% language model, and the entries in the substitute distribution
%% represent the probability of those words being used in the given
%% context.  

% how are the substitutes computed
It is best to use both the left and the right context when estimating
the probabilities for potential lexical substitutes.  For example, in
\emph{``He lived in San Francisco suburbs.''}, the token \emph{San}
would be difficult to guess from the left context but it is almost
certain looking at the right context.  We define $c_w$ as the $2n-1$
word window centered around the target word position: $w_{-n+1} \ldots
w_0 \ldots w_{n-1}$ ($n=4$ is the n-gram order we have used).  The
probability of a substitute word $w$ in a given context $c_w$ can be
estimated as:
\begin{eqnarray}
  \label{eq:lm1}P(w_0 = w | c_w) & \propto & P(w_{-n+1}\ldots w_0\ldots w_{n-1})\\
  \label{eq:lm2}& = & P(w_{-n+1})P(w_{-n+2}|w_{-n+1})\nonumber\\
  &&\ldots P(w_{n-1}|w_{-n+1}^{n-2})\\
  \label{eq:lm3}& \approx & P(w_0| w_{-n+1}^{-1})P(w_{1}|w_{-n+2}^0)\nonumber\\
  &&\ldots P(w_{n-1}|w_0^{n-2})
\end{eqnarray}
where $w_i^j$ represents the sequence of words $w_i w_{i+1} \ldots
w_{j}$.  In Equation \ref{eq:lm1}, $P(w|c_w)$ is proportional to
$P(w_{-n+1}\ldots w_0 \ldots w_{n-1})$ because the words of the
context are fixed.  Terms without $w_0$ are identical for each
substitute in Equation \ref{eq:lm2} therefore they have been dropped
in Equation \ref{eq:lm3}.  Finally, because of the Markov property of
n-gram language model, only the closest $n-1$ words are used in the
experiments.

Near the sentence boundaries the appropriate terms were truncated in
Equation \ref{eq:lm3}.  Specifically, at the beginning of the sentence
shorter n-gram contexts were used and at the end of the sentence terms
beyond the end-of-sentence token were dropped.

To obtain a discrete representation of the context, the
random-substitutes algorithm pairs each word token with a substitute
sampled from the pre-computed substitute distribution generated from
the word token's context and then word ($W$) -- random-substitute
($S$) pairs are fed to the S-CODE algotihm as input.

\appendixsection{The CODE Model}
\label{app:codethr}

In this section we review the unsupervised method that we use to model
co-occurrence statistics: the Co-occurrence Data Embedding
(CODE)\ \cite{globerson2007euclidean} method and its spherical
extension (S-CODE) introduced by \cite{maron2010sphere}.

Let $W$ and $C$ be two categorical variables with finite cardinalities
$|W|$ and $|C|$.  We observe a set of pairs $\{w_i, c_i\}_{i=1}^n$
drawn IID from the joint distribution of $W$ and $C$.  The basic idea
behind CODE and related methods is to represent (embed) each value of
$W$ and each value of $C$ as points in a common Euclidean space
$\mathbf{R}^d$ such that values that frequently co-occur lie close to
each other.  There are several ways to formalize the relationship
between the distances and co-occurrence statistics, in this paper we
use the following:
\begin{equation} \label{eq:probability}
p(w,c) = \frac{1}{Z} \bar{p}(w) \bar{p}(c) e^{-d^2_{w,c}}
\end{equation}
\noindent where $d^2_{w,c}$ is the squared distance between the
embeddings of $w$ and $c$, $\bar{p}(w)$ and $\bar{p}(c)$ are empirical
probabilities, and $Z=\sum_{w,c} \bar{p}(w) \bar{p}(c) e^{-d^2_{w,c}}$
is a normalization term.  If we use the notation $\phi_w$ for the
point corresponding to $w$ and $\psi_c$ for the point corresponding to
$c$ then $d^2_{w,c} = \|\phi_w-\psi_c\|^2$.  The log-likelihood of a
given embedding $\ell(\phi, \psi)$ can be expressed as:
\begin{eqnarray}
&&\ell(\phi, \psi) = \sum_{w,c} \bar{p}(w,c) \log p(w,c) \label{eq:likelihood} \\
&&= \sum_{w,c} \bar{p}(w,c) (-\log Z + \log \bar{p}(w)\bar{p}(c) - d^2_{w,c}) \nonumber \\
&&= -\log Z + \mathit{const} - \sum_{w,c} \bar{p}(w,c) d^2_{w,c} \nonumber
\end{eqnarray}
The likelihood is not convex in $\phi$ and $\psi$.  We use gradient
ascent to find an approximate solution for a set of $\phi_w$, $\psi_c$
that maximize the likelihood.  The gradient of the $d^2_{w,c}$ term
pulls neighbors closer in proportion to the empirical joint
probability:
\begin{equation}
\frac{\partial}{\partial\phi_w} \sum_{w,c} -\bar{p}(w,c) d^2_{w,c} =
\sum_y 2 \bar{p}(w,c) (\psi_c - \phi_w) \label{eq:attract}
\end{equation}
The gradient of the $Z$ term pushes neighbors apart in proportion to the
estimated joint probability:
\begin{equation}
\frac{\partial}{\partial\phi_x} (-\log Z) = \sum_y 2 p(w,c) (\phi_w -
\psi_c) \label{eq:repulse}
\end{equation}
Thus the net effect is to pull pairs together if their estimated
probability is less than the empirical probability and to push them
apart otherwise.  The gradients with respect to $\psi_c$ are similar.
S-CODE \cite{maron2010sphere} additionally restricts all $\phi_w$ and
$\psi_c$ to lie on the unit sphere.  With this restriction, $Z$ stays
around a fixed value during gradient ascent.  This allows S-CODE to
substitute an approximate constant $\tilde{Z}$ in gradient
calculations for the real $Z$ for computational efficiency.  In our
experiments, we used S-CODE with its sampling based stochastic
gradient ascent algorithm and smoothly decreasing learning rate.

\appendixsection{S-CODE with More than Two Variables}

In order to accommodate multiple feature types the S-CODE model in the
previous section needs to be extended to handle more than two
variables.  Globerson et. al \shortcite{globerson2007euclidean}
suggest the following likelihood function:

\begin{eqnarray}
&\ell(\phi,& \psi^{(1)}, \ldots, \psi^{(K)}) = \label{eq:multicode}
  \bar{p}(w,c) \log p(w,c) + \sum_i^K \sum_{w,f^{(i)}} \bar{p}(w,f^{(i)}) \log p(w,f^{(i)})
\end{eqnarray}

\noindent where $\bar{p}(w,c)$ is the empirical joint distribution of
context $C$ with $W$, $F^{(1)}, \ldots, F^{(K)}$ are extra $K$
different variables whose empirical joint distributions with $W$,
$\bar{p}(w,f^{(1)})\ldots\bar{p}(w,f^{(K)})$, are known.
Eq.~\ref{eq:multicode} then represents a set of CODE models
$p(w,f^{(k)})$ where each $F^{(k)}$ has an embedding $\psi_f^{(k)}$
but all models share the same $\phi_w$ embedding.

We adopt this likelihood function, let $W$ represent a word, $C$
represent a context (i.e., random substitute), and $F^{(1)}, \ldots,
F^{(K)}$ stand for morphological and orthographic features of the word
thus each co-occurrence is a (K+1)-tuple, $(W, C, F^{(1)}, \hdots
F^{(K)})$.  With this setup, the training procedure needs to change
little: instead of sampling a word ($w$) -- context ($c$), the word
($w$) -- context ($c$) -- features ($f_1,\hdots,f_K$) tuple is sampled
and input to the gradient ascent algorithm.  The gradient search
algorithm updates the embeddings according to $p(w,c)$ and
$p(w,f^{(i)})$ where $i=1\hdots k$ and no updates are performed
between $c$ and $f^{(i)}$s since they do not have any co-occurrence
statistics and $w$ is the only shared variable.

Tuples might have null values due to unobserved features.  For example
in the case of POS induction, the word ``\textbf{car}'' has no
morphological or orthographic features therefore all the elements of
the tuple have null value except the word type ($w$) and the context
($c$).  We do not perform any pull or push updates on embeddings
during the gradient search if the corresponding $f^{(k)}$ is
null\footnote{In the POS induction problem $w$ and $c$ represents the
  word type and context therefore they are always observed.}.

\newpage
\appendixsection{Language Statistics}
Table~\ref{tab:lmstatistics} summarizes statistics of the training
corpora of language model and the test corpora for each language in
the test set.  
\input{datatable.tex}
%% S-CODE handles two variables, whereas underlying syntactic categories
%% can be captured by more than two different variables such as
%% contextual, morphologic and ortographic features.  S-CODE can be
%% extented to handle more than two variables in a way similar to the
%% multi variable extension of CODE \cite{globerson2007euclidean} with
%% the unit sphere restriction.  The log-likelihood at
%% Equation~\ref{eq:likelihood} can be redefined for $n+1$ different
%% categorical variables $X$, $Y_i$, $\hdots$ and $Y_n$ with finite
%% cardinalities $|X|$, $|Y_1|$, $\hdots$ and $|Y_n|$, respectively, as:
%% \begin{eqnarray}
%% &&\ell(\phi, \psi_1,\hdots,\psi_n) = \sum_{i=1}^n\sum_{x,y_i} \bar{p}(x,y_i) \log p(x,y_i) \label{eq:multiscode} \\
%% &&= \sum_{i=1}^n\sum_{x,y_i} \bar{p}(x,y_i) (-\log Z_i + \log \bar{p}(x)\bar{p}(y_i) - d^2_{x,y_i}) \nonumber \\
%% &&=-\sum_{i=1}^n(\log Z_i + \mathit{const}_i - \sum_{x,y_i} \bar{p}(x,y_i) d^2_{x,y_i}) \nonumber
%% \end{eqnarray}
%% where $\psi_i$ is the embedding of $y_i \in Y_i$ and $Z_i$ is the
%% normalization term of $p(x,y_i)$.  Thus the model is able to jointly
%% learn the embeddings when the pairwise co-occurence statistics,
%% $\bar{p}(x,y_i)$, are available for all $i$.

%% One problem with these setting is, not every $(x,y_i)$ pair is
%% observed in the data.  For example, the stem word ``\textbf{car}''
%% doesn't have any morphological feature, thus its morphological feature
%% is represented by a null value, ``X''.  However setting the unobserved
%% features to ``X'' leads to pulling the words with unobserved features
%% together even they are from different clusters or pushing the ones
%% with observed features apart even they are from same clusters.  To
%% solve this, during the gradient search we don't perform any pull or
%% push updates on embeddings if the value of $y_i$ is set to null.
