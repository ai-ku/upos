\subsection{Multilingual Experiments}
\label{sec:multilang}
\noindent We performed experiments with a range of languages and three
different feature configurations to establish both the robustness of
our model across languages and to observe the effects of different
features.  Following Christodoulopoulos et
al. \shortcite{christodoulopoulos-goldwater-steedman:2011:EMNLP}, in
addition to the PTB we extend our experiments to 8 languages from
MULTEXT-East (Bulgarian, Czech, English, Estonian, Hungarian,
Romanian, Slovene and Serbian) \cite{citeulike:5820223} and 10
languages from the CoNLL-X shared task (Bulgarian, Czech, Danish,
Dutch, German, Portuguese, Slovene, Spanish, Swedish and Turkish)
\cite{Buchholz:2006:CST:1596276.1596305}.  For all experiments, we use
the best performing model of Section~\ref{sec:clustering-w} (i.e.
clustering the word embeddings) with default settings.  To perform
meaningful comparisons with the previous work we train and evaluate
our models on the training section of MULTEXT-East\footnote{Languages
  of MULTEXT-East corpora do not tag the punctuation marks, thus we
  add an extra tag for punctuation to the tag-set of these languages.}
and CONLL-X languages \cite{Lee:2010:STU:1870658.1870741}.

% Section~\ref{sec:multivecfeat} details the language model and feature
% extraction for each language.  Section~\ref{sec:multires} summarizes
% the results of our models for all of the languages in our corpora.  In
% the rest of this section we refer to the MULTEXT-East and CONLL-X
% corpora as the testing corpora and the language model training corpora
% as the training corpora.

% \subsubsection{Random Substitutes and Features}
% \label{sec:multivecfeat}

%% Language model and feature extraction
We train a 4-gram language model with the corresponding training
corpora of each language as described in Section~\ref{sec:expset}.  To
sample substitutes we calculate the probabilities of the top 100
substitutes for each position by using the corresponding language
model.  Morphological features of each language are extracted by the
method described in Section~\ref{sec:feat}.  The details of the
language model training and feature extraction are detailed in
Appendix~D.
\input{multibesttable.tex}
%% We ignore these results
%% \input{tokentable.tex}
%% \subsubsection{Results}
%% \label{sec:multires}
For each language we report results of three models that cluster: (1)
word embeddings ({\em CLU-W}), (2) word embeddings with orthographic
features ({\em CLU-W+O}) and (3) word embeddings with both orthographic
and morphological features ({\em CLU-W+O+M}).  Similar to the settings
used in Section~\ref{sec:clustering-w}, we use the 25 dimensional
sphere with 64 substitutes for all languages.  For each language the
number of induced clusters is set to the number of tags in the
gold-set as presented in Table~\ref{tab:multiresults}.

As a baseline model we chose the syntagmatic bigram version of S-CODE
described in Section~\ref{sec:pvss} which is a very strong baseline
compared to the ones used in
\cite{christodoulopoulos-goldwater-steedman:2011:EMNLP}.
Table~\ref{tab:multiresults} summarizes the \mto\ and \vm\ scores of
our models together with the syntagmatic bigram baseline and the best
published accuracies on each language corpus.

{\em CLU-W} significantly outperforms the syntagmatic bigram baseline
in both \mto\ and \vm\ scores on 14 languages.  {\em CLU-W+O+M} has
the state-of-the-art \mto\ and \vm\ accuracy on the PTB.  {\em
  CLU-W+O} and {\em CLU-W+O+M} achieve the highest \mto\ scores on all
languages of MULTEXT-East corpora while scoring the highest \vm\
accuracies on English and Romanian.  On the CoNLL-X languages our
models perform better than the best published \mto\ or \vm\ accuracies
on 10 languages.

