\subsection{Multilingual Experiments}
\label{sec:multilang}
\noindent We performed experiments with a range of languages and three
different feature configurations to establish both the robustness of
our model across languages and to observe the effects of different
features.  Following Christodoulopoulos et
al. \shortcite{christodoulopoulos-goldwater-steedman:2011:EMNLP}, in
addition to the PTB we extend our experiments to 8 languages from
MULTEXT-East (Bulgarian, Czech, English, Estonian, Hungarian,
Romanian, Slovene and Serbian) \cite{citeulike:5820223} and 10
languages from the CoNLL-X shared task (Bulgarian, Czech, Danish,
Dutch, German, Portuguese, Slovene, Spanish, Swedish and Turkish)
\cite{Buchholz:2006:CST:1596276.1596305}.  For all experiments, we use
the best performing model of Section~\ref{sec:clustering-w} (i.e.
clustering the word embeddings) with default settings.  To perform
meaningful comparisons with the previous work we train and evaluate
our models on the training section of MULTEXT-East\footnote{Languages
  of MULTEXT-East corpora do not tag the punctuation marks, thus we
  add an extra tag for punctuation to the tag-set of these languages.}
and CONLL-X languages \cite{Lee:2010:STU:1870658.1870741}.

% Section~\ref{sec:multivecfeat} details the language model and feature
% extraction for each language.  Section~\ref{sec:multires} summarizes
% the results of our models for all of the languages in our corpora.  In
% the rest of this section we refer to the MULTEXT-East and CONLL-X
% corpora as the testing corpora and the language model training corpora
% as the training corpora.

% \subsubsection{Random Substitutes and Features}
% \label{sec:multivecfeat}

%% Language model and feature extraction
We train a 4-gram language model with the corresponding training
corpora of each language as described in Section~\ref{sec:expset}.  To
sample substitutes we calculate the probabilities of the top 100
substitutes for each position by using the corresponding language
model.  Morphological features of each language are extracted by the
method described in Section~\ref{sec:feat}.  The details of the
language model training and feature extraction are detailed in
Appendix~D.

%% We ignore these results
%% \input{tokentable.tex}
%% \subsubsection{Results}
%% \label{sec:multires}
\input{multibesttable.tex}
For each language we report results of three models: (1) word
embeddings ($W$), (2) word embeddings with orthographic features
($W+O$) and (3) word embeddings with both orthographic and
morphological features ($W+O+M$).  Similar to the settings used in
Section~\ref{sec:clustering-w}, we use the 25 dimensional sphere with 64
substitutes for all languages.  For each language the number of
induced clusters is set to the number of tags in the gold-set as
presented in Table~\ref{tab:multiresults}.

As a baseline model we chose the syntagmatic bigram version of S-CODE
described in Section~\ref{sec:bigram} which is a very strong baseline
compared to the ones used in
\cite{christodoulopoulos-goldwater-steedman:2011:EMNLP}.
Table~\ref{tab:multiresults} summarizes the \mto\ and \vm\ scores of
our models together with the syntagmatic bigram baseline and the best
published accuracies on each language corpus.

$W$ significantly outperforms the syntagmatic bigram baseline in both
\mto\ and \vm\ scores on 14 languages.  $W+O+M$ has the
state-of-the-art \mto\ and \vm\ accuracy on the PTB.  $W+O$ and
$W+O+M$ achieve the highest \mto\ scores on all languages of
MULTEXT-East corpora while scoring the highest \vm\ accuracies on
English and Romanian.  On the CoNLL-X languages our models perform
better than the best published \mto\ or \vm\ accuracies on 10
languages.

