\begin{abstract}
\paragraph{ACL}
We introduce a paradigmatic representation of word context and
demonstrate its utility in learning syntactic categories.  Unlike the
typical syntagmatic representations of word context which consist of
properties of neighboring words, our paradigmatic representation
consists of substitute vectors: possible substitutes of the target
word and their probabilities.  When word contexts are clustered based
on their substitute vectors they reveal a grouping that largely match
the traditional part of speech boundaries with a many-to-one accuracy
of \collapseResult\% on a 45-tag 24K word test corpus.
\paragraph{EMNLP}
We investigate paradigmatic representations of word context in the
domain of unsupervised syntactic category acquisition.  Paradigmatic
representations of word context are based on potential substitutes of
a word in contrast to syntagmatic representations based on properties
of neighboring words.  We compare a bigram based baseline model with
several paradigmatic models and demonstrate significant gains in
accuracy.  Our best model based on Euclidean co-occurrence embedding
combines the paradigmatic context representation with morphological
and orthographic features and achieves 80\% many-to-one accuracy on a
45-tag 1M word corpus.
\end{abstract}

