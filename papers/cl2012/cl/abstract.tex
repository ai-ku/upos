\begin{abstract}
%% \paragraph{ACL}
%% We introduce a paradigmatic representation of word context and
%% demonstrate its utility in learning syntactic categories.  Unlike the
%% typical syntagmatic representations of word context which consist of
%% properties of neighboring words, our paradigmatic representation
%% consists of substitute vectors: possible substitutes of the target
%% word and their probabilities.  When word contexts are clustered based
%% on their substitute vectors they reveal a grouping that largely match
%% the traditional part of speech boundaries with a many-to-one accuracy
%% of \collapseResult\% on a 45-tag 24K word test corpus.
%% \paragraph{EMNLP}

We investigate paradigmatic representations of word context in the
domain of unsupervised part of speech induction.  Paradigmatic
representations of word context are based on potential substitutes of
a word in contrast to syntagmatic representations based on properties
of neighboring words.  We demonstrate paradigmatic representations
within two frameworks: (1) context clustering and (2) co-occurrence
modeling.  In context clustering we cluster word contexts based on
the potential substitutes and they reveal a grouping that largely
match the traditional part of speech boundaries.  In co-occurrence
modelling we construct a Euclidean embedding that models the
co-occurrence of word types and their contexts.  Clustering the points
that correspond to word types in the Euclidean embedding gives
state-of-the-art results in unsupervised part of speech induction,
including 80\% many-to-one accuracy on the Penn Treebank and
significant improvements on 17 out of 19 corpora in 15 languages.

\end{abstract}


% LocalWords:  syntagmatic occurance WSJ
