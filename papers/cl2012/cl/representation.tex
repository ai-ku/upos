\section{Representing Word Context}
\label{sec:representation}

% example substitute vectors (both syntactic and semantic)

In this section we demonstrate the different contextual
representations in the part-of-speech induction (aka. syntactic word
categorization) literature and introduce the substitute words as an
alternative to the current context representations.  In the rest of
the paper the words in the vocabulary are referred as {\em types} and
the instances of types are referred as {\em tokens}.

The contextual representations can be categorized into three groups
based on the way they incorporate the local context information of the
target type or token: (1) syntagmatic representation, (2) Hidden
Markov Models (HMM) and (3) paradigmatic representation.  These
representations can be further subdivided into two subgroups based on
whether they group the types or the tokens.

%\subsection{Co-occurrences}
\paragraph{Syntagmatic Representation}

The syntagmatic representation represents the context using the
neighboring words, typically co-occurrences with a single word on the
left or a single word on the right word called a ``frame'' (e.g., {\em
  {\bf the} dog {\bf is}; {\bf the} cat {\bf is}}).
\cite{SchutzePe93,redington1998distributional,mintz2003frequent,20674613,lamar-EtAl:2010:Short,maron2010sphere}.
Turney and Pantel \shortcite{DBLP:journals/jair/TurneyP10} give a
broad overview of syntagmatic approaches and their applications within
the Vector Space Modeling framework.  Depending on the way they
incorporate co-occurences these models can group types or tokens.

Sch\"{u}tze \shortcite{SchutzePe93} represented the context of a word
type by concatenating its left and right co-occurences vectors.  These
vectors were calculated for each type by using the left and the right
neighbors of the type instances therefore they characterize the
distribution of the left and right neighboring tokens of the type.
One constraint of this representation is that it represents types
rather than tokens thus it is not possible to group the instances of
any type into the separate categories.

Mintz \shortcite{mintz2003frequent} showed on a subset of child
directed speech corpus (CHILDES) \cite{macwhinney2000childes} that
non-adjacent high frequent bigram frames are useful for the language
learners on the syntactic categorization of the tokens.  For example,
the tokens that are observed at ``\_'' in the frame ``{\em {\bf the} \_
  {\bf is}}'' are assigned to the same category.  Using the top-45
frequent frames Mintz achieved an average of 98\% unsupervised
accuracy\footnote{Unsupervised accuracy was defined as the number of
  hits (when two intervening tokens observed in the frame from the
  same category) divided by number of false alarms (when two
  intervening tokens observed in the frame from different
  categories).}.  The main limitation of the top-45 frequent frames is
that they could only analyze the 6\% of the tokens on average due to
the sparsity.  Another drawback is that the tokens with only one
common neighbors could not exchange information.

St Clair et al.  \shortcite{20674613} extended the work of Mintz
\shortcite{mintz2003frequent} and introduced the flexible bigram
frames which represent the context by using the left and the right
bigrams separately.  As a result tokens with a common left or right
bigram can exchange information and might be grouped together.  For
instance, two tokens that are observed at ```\_'' in ``{\em {\bf the}
  \_ {\bf is}}'' and ``{\em {\bf a} \_ {\bf is}}'' can be categorized
together due to the shared right bigram ``{\em{\bf is}}''.  Using a
feed forward connectionist model they showed that the flexible frames
are statistically better than the frequent frames in terms of the
supervised accuracy\footnote{The number of analyzed tokens was same in
  their experiments since they used all the frequent frames instead of
  the top-45 ones.}.  They also showed that representing token
contexts only with the left or the right bigram is statistically
better than the frequent frames but worse than the flexible frames in
terms of supervised accuracy.  Both Mintz
\shortcite{mintz2003frequent} and St Clair \shortcite{20674613} did
not report any results with contexts larger than bigram since as the
context is enriched, the re-occurrence frequency of a frame becomes
lower which causes the data sparsity \cite{manning99foundations}.

%\subsection{HMM}
\paragraph{HMM} 
Prototypical HMM uses a bigram structure where tokens are generated by
latent categories and learns the latent category sequence that
generates the given word sequence instead of clustering token directly
\cite{Brown:1992:CNG:176313.176316,blunsom-cohn:2011:ACL-HLT2011,goldwater-griffiths:2007:ACLMain,johnson:2007:EMNLP-CoNLL2007,Ganchev:2010:PRS:1859890.1859918,bergkirkpatrick-klein:2010:ACL,Lee:2010:STU:1870658.1870741}.
As a result The POS induction literature focused on the first and
second order HMMs since the higher order HMMs have additional
complicating factors\footnote{The number of parameters in a
  prototypical HMM quadratically increases as the HMM order
  increases.}  and require more complex training procedures
\cite{johnson:2007:EMNLP-CoNLL2007}.  Depending on the design and the
training procedure HMM models can be trained to cluster types or
tokens which are detailed in Section \ref{sec:related}.

%\subsection{Substitute words}
\paragraph{Paradigmatic Representation} 

In the paradigmatic representation context is defined as the
distribution of the substitute words in that context.  Sch\"{u}tze
\shortcite{Schutze:1995:DPT:976973.976994} incorporated paradigmatic
information by concatenating the left co-occurrence and the right
co-occurrence vectors of the right and the left tokens, respectively
and grouped the tokens that have similar vectors.  Yatbaz et
al. \shortcite{yatbaz-sert-yuret:2012:EMNLP-CoNLL} calculated the most
likely substitute words of a word in a given context and grouped the
types that have similar substitutes.

Our paradigmatic representation is also related to the second order
co-occurrences used in \cite{Schutze:1995:DPT:976973.976994}.
Sch{\"u}tze concatenates the left and right context vectors for the
target word type with the left context vector of the right neighbor
and the right context vector of the left neighbor.  The vectors from
the neighbors include potential substitutes.  Our method improves on
his foundation from three aspects: (1) it can cluster both the types
and tokens (2) it uses a 4-gram language model rather than bigram
statistics, (3) it includes the whole 78,498 word vocabulary rather
than the most frequent 250 words.  More importantly, rather than
simply concatenating the vectors that represent the target word with
vectors that represent the context we use a co-occurrence modeling
algorithm.

Similarly, Sch{\"u}tze and Pedersen \shortcite{SchutzePe93} define the
words that frequently co-occur together as the {\em syntagmatic
  associates} and words that have similar left and right neighbors as
the {\em paradigmatic parallels}.  Turney and Pantel
\shortcite{DBLP:journals/jair/TurneyP10} give a broad overview of
syntagmatic approaches and their applications within the Vector Space
Modeling framework.  We find that representing the paradigmatic axis
more directly using substitute vectors rather than frequent neighbors
improves part of speech induction.

Sahlgren \shortcite{sahlgren2006word} gives a detailed analysis of
paradigmatic and syntagmatic relations in the context of word-space
models used to represent the word meanings.  Sahlgren's paradigmatic
model represents word types using co-occurrence counts of their
frequent neighbors, in contrast to his syntagmatic model that
represents word types using counts of contexts (documents, sentences)
they occur in.  Our substitute vectors do not represent word types at
all, but {\em contexts of word tokens} using probabilities of likely
substitutes.  Sahlgren finds that in word-spaces built by frequent
neighbor vectors, more nearest neighbors share the same part of speech
compared to word-spaces built by context vectors.

The two examples below illustrate the advantage of paradigmatic
representations in uncovering similarities where no overt similarity
that can be captured by a syntagmatic representation exists.  The word
``board'' from the first sentence and the word ``council'' from the
second sentence have no common neighbors except the determiner
``the''.  The paradigmatic representation captures the similarity of
these words by suggesting the same top substitutes for both (the
numbers in parentheses give substitute probabilities):
\begin{quote}
 (1) \noindent{\em ``Pierre Vinken, 61 years old, will join the {\bf board} as a nonexecutive director Nov.~29.''}\\
% {\bf the:} its (.9011), the (.0981), a (.0006), $\ldots$\\
 {\bf board:} board (.4288), company (.2584), firm (.2024), bank (.0731), $\ldots$
\end{quote}

\begin{quote}
%  (2) \noindent {\em ``Blacks and Hispanics currently make up 38 \% of the city 's population and hold only 25 \% of the seats on the {\bf council} .''}\\
  (2) \noindent {\em ``$\ldots$ and hold only 25 \% of the seats on the {\bf council} .''}\\
 \noindent {\bf council:} board (.6591), company (.0795), firm (.0542), bank (.0154), $\ldots$
\end{quote}

The high probability substitutes reflect both semantic and syntactic
properties of the context.  Top substitutes for ``board'' and
``council'' are not only nouns, but specifically nouns compatible with
the semantic context.  Top substitutes for the word ``the'' in the
first example consist of words that can act as determiners: its
(.9011), the (.0981), a (.0006), $\ldots$

%% \subsection{Substitute Vectors}
%% \label{sec:subthr}
%% %%substitute vector applications

%% In this section we explore the usage of substitute vectors within the
%% context clustering framework by comparing similarity metrics,
%% dimensionality reduction techniques and clustering methods.  The first
%% subsection describes the computation of substitute vectors using a
%% statistical language model.  Section~\ref{sec:dist} gives a detailed
%% comparison of similarity metrics in the high dimensional substitute
%% vector space.  Section~\ref{sec:dimreduce} analyzes the application of
%% dimensionality reduction algorithms to substitute vectors.
%% Section~\ref{sec:clustering} presents a comparison of clustering
%% methods on the PTB.

\subsection{Computation of Substitute Vectors}
\label{sec:subcomp}

In this study, we predict the syntactic category of a word in a given
context based on its substitute vector.  The dimensions of the
substitute vector represent words in the vocabulary, and the entries
in the substitute vector represent the probability of those words
being used in the given context.  Note that the substitute vector is a
function of the context only and is indifferent to the target word.

%This section details the choice of the data set, the vocabulary and
%the estimation of substitute vector probabilities.

%% % what is the test data
%% The Wall Street Journal Section of the Penn Treebank \cite{treebank3}
%% was used as the test corpus (1,173,766 tokens, 49,206 types).
%% % what is the tag set
%% The treebank uses 45 part of speech tags which is the set we used as
%% the gold standard for comparison in our experiments.
%% % what is the LM training data
%% %Train => 5181717 126019973 690121813
%% To compute substitute probabilities we trained a language model using
%% approximately 126 million tokens of Wall Street Journal data
%% (1987-1994) extracted from CSR-III Text \cite{csr3text} (we excluded
%% the test corpus).
%% % how is the language model trained
%% We used SRILM \cite{Stolcke2002} to build a 4-gram language model with
%% Kneser-Ney discounting.
%% % what is the vocabulary
%% Words that were observed less than 20 times in the language model
%% training data were replaced by \textsc{unk} tags, which gave us a
%% vocabulary size of 78,498.
%% % perplexity
%% The perplexity of the 4-gram language model on the test corpus is 96.

% how are the substitutes computed
It is best to use both the left and the right context when estimating the
probabilities for potential lexical substitutes.  For example, in
\emph{``He lived in San Francisco suburbs.''}, the token \emph{San}
would be difficult to guess from the left context but it is almost
certain looking at the right context.  We define $c_w$ as the $2n-1$
word window centered around the target word position: $w_{-n+1} \ldots
w_0 \ldots w_{n-1}$ ($n=4$ is the n-gram order).  The probability of a
substitute word $w$ in a given context $c_w$ can be estimated as:
\begin{eqnarray}
  \label{eq:lm1}P(w_0 = w | c_w) & \propto & P(w_{-n+1}\ldots w_0\ldots w_{n-1})\\
  \label{eq:lm2}& = & P(w_{-n+1})P(w_{-n+2}|w_{-n+1})\nonumber\\
  &&\ldots P(w_{n-1}|w_{-n+1}^{n-2})\\
  \label{eq:lm3}& \approx & P(w_0| w_{-n+1}^{-1})P(w_{1}|w_{-n+2}^0)\nonumber\\
  &&\ldots P(w_{n-1}|w_0^{n-2})
\end{eqnarray}
where $w_i^j$ represents the sequence of words $w_i w_{i+1} \ldots
w_{j}$.  In Equation \ref{eq:lm1}, $P(w|c_w)$ is proportional to
$P(w_{-n+1}\ldots w_0 \ldots w_{n-1})$ because the words of the
context are fixed.  Terms without $w_0$ are identical for each
substitute in Equation \ref{eq:lm2} therefore they have been dropped
in Equation \ref{eq:lm3}.  Finally, because of the Markov property of
n-gram language model, only the closest $n-1$ words are used in the
experiments.

Near the sentence boundaries the appropriate terms were truncated in
Equation \ref{eq:lm3}.  Specifically, at the beginning of the sentence
shorter n-gram contexts were used and at the end of the sentence terms
beyond the end-of-sentence token were dropped.  

%% Rest of this section details the choice of the data set, the
%% vocabulary and the estimation of substitute probabilities.
%% For computational efficiency only the top 100 substitutes and their
%% unnormalized probabilities were computed for each of the 1,173,766
%% positions in the test set\footnote{The substitutes with unnormalized
%%   log probabilities can be downloaded from
%%   \mbox{\url{http://goo.gl/jzKH0}}.  For a description of the {\sc
%%     fastsubs} algorithm used to generate the substitutes please see
%%   \mbox{\url{http://arxiv.org/abs/1205.5407v1}}.  {\sc fastsubs}
%%   accomplishes this task in about 5 hours, a naive algorithm that
%%   looks at the whole vocabulary would take more than 6 days on a
%%   typical 2012 workstation.}.  The probability vectors for each
%% position were normalized to add up to 1.0 giving us the final
%% substitute vectors used in the rest of this study.

% what is the test data
The Wall Street Journal Section of the Penn Treebank (PTB) \cite{treebank3}
was used as the test corpus (1,173,766 tokens, 49,206 types) to be
induced.
% what is the tag set
The treebank uses 45 part-of-speech tags which is the set we used as
the gold standard for comparison in our experiments.
% what is the LM training data
%Train => 5181717 126019973 690121813
To compute substitute probabilities we trained a language model using
approximately 126 million tokens of Wall Street Journal data
(1987-1994) extracted from CSR-III Text \cite{csr3text} (excluding
sections of the PTB).
% how is the language model trained
We used SRILM \cite{Stolcke2002} to build a 4-gram language model with
Kneser-Ney discounting.
% what is the vocabulary
Words that were observed less than 20 times in the language model
training data were replaced by \textsc{unk} tags, which gave us a
vocabulary size of 78,498.
% what is the test data
% perplexity
The perplexity of the 4-gram language model on the test corpus is 96.
