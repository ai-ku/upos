\section{Co-occurrence Modeling}
\label{sec:code}

The general strategy we follow for unsupervised syntactic category
acquisition is to combine features of the context with the identity
and features of the target word.  Our preliminary experiments in
Section~\ref{sec:clustering} indicated that using the context
information alone (e.g. clustering substitute vectors) without the
target word identity and features had limited success.  Moreover
incorporating word identities (i.e. one-tag-per-word constraint) even
in an ad-hoc manner by collapsing the clustering output significantly
improves the \mto accuracy.  Thus it is the co-occurrence of a target
word with a particular type of context that best predicts the
syntactic category.  

In this section we present applications of substitute vectors as
representations of word context within the Co-occurrence Data
Embedding (CODE)\cite{globerson2007euclidean} framework.
Section~\ref{sec:codethr} reviews the unsupervised methods we used to
model co-occurrence statistics: the CODE method and its spherical
extension (S-CODE) introduced by \cite{maron2010sphere}.  The S-CODE
algorithm works with discrete inputs.  The substitute vectors as
described in Section~\ref{sec:subthr} are high dimensional and
continuous.  We experimented with two approaches to use substitute
vectors in a discrete setting using the default parameters defined in
Section~\ref{sec:expset}.  Section~\ref{sec:rpart} presents an
algorithm that partitions the high dimensional space of substitute
vectors into small neighborhoods and uses the partition id as a
discrete context representation.  Section~\ref{sec:wordsub} presents
an even simpler model which pairs each word with random substitutes
sampled from the substitute vector.  Section~\ref{sec:bigram}
replicates the bigram based S-CODE results from \cite{maron2010sphere}
as a direct comparison of paradigmatic vs syntagmatic representations
of word context.  When the left-word -- right-word pairs used in the
bigram model are replaced with word -- partition-id or word --
substitute pairs we see significant gains in accuracy.  These results
support our running hypothesis that paradigmatic features,
i.e. potential substitutes of a word, are better determiners of
syntactic category compared to left and right neighbors (syntagmatic
features).  Section~\ref{sec:feat} explores morphological and
orthographic features as additional sources of information and its
results improve the state-of-the-art in the field of unsupervised
syntactic category acquisition.

\subsection{The CODE Model}
\label{sec:codethr}

Let $X$ and $Y$ be two categorical variables with finite cardinalities
$|X|$ and $|Y|$.  We observe a set of pairs $\{x_i, y_i\}_{i=1}^n$
drawn IID from the joint distribution of $X$ and $Y$.  The basic idea
behind CODE and related methods is to represent (embed) each value of
$X$ and each value of $Y$ as points in a common low dimensional
Euclidean space $\mathbf{R}^d$ such that values that frequently
co-occur lie close to each other.  There are several ways to formalize
the relationship between the distances and co-occurrence statistics, in
this paper we use the following:
\begin{equation} \label{eq:probability}
p(x,y) = \frac{1}{Z} \bar{p}(x) \bar{p}(y) e^{-d^2_{x,y}}
\end{equation}
\noindent where $d^2_{x,y}$ is the squared distance between the
embeddings of $x$ and $y$, $\bar{p}(x)$ and $\bar{p}(y)$ are empirical
probabilities, and $Z=\sum_{x,y} \bar{p}(x) \bar{p}(y) e^{-d^2_{x,y}}$ is
a normalization term.  If we use the notation $\phi_x$ for the
point corresponding to $x$ and $\psi_y$ for the point corresponding
to $y$ then $d^2_{x,y} = \|\phi_x-\psi_y\|^2$.  The log-likelihood
of a given embedding $\ell(\phi, \psi)$ can be expressed as:
\begin{eqnarray} 
&&\ell(\phi, \psi) = \sum_{x,y} \bar{p}(x,y) \log p(x,y) \label{eq:likelihood} \\
&&= \sum_{x,y} \bar{p}(x,y) (-\log Z + \log \bar{p}(x)\bar{p}(y) - d^2_{x,y}) \nonumber \\
&&= -\log Z + \mathit{const} - \sum_{x,y} \bar{p}(x,y) d^2_{x,y} \nonumber
\end{eqnarray}
The likelihood is not convex in $\phi$ and $\psi$.  We use gradient
ascent to find an approximate solution for a set of $\phi_x$, $\psi_y$
that maximize the likelihood.  The gradient of the $d^2_{x,y}$ term
pulls neighbors closer in proportion to the empirical joint
probability:
\begin{equation}
\frac{\partial}{\partial\phi_x} \sum_{x,y} -\bar{p}(x,y) d^2_{x,y} =
\sum_y 2 \bar{p}(x,y) (\psi_y - \phi_x) \label{eq:attract}
\end{equation}
The gradient of the $Z$ term pushes neighbors apart in proportion to the
estimated joint probability:
\begin{equation}
\frac{\partial}{\partial\phi_x} (-\log Z) = \sum_y 2 p(x,y) (\phi_x -
\psi_y) \label{eq:repulse}
\end{equation}
Thus the net effect is to pull pairs together if their estimated
probability is less than the empirical probability and to push them
apart otherwise.  The gradients with respect to $\psi_y$ are
similar.

S-CODE \cite{maron2010sphere} additionally restricts all $\phi_x$ and
$\psi_y$ to lie on the unit sphere.  With this restriction, $Z$ stays
around a fixed value during gradient ascent.  This allows S-CODE to
substitute an approximate constant $\tilde{Z}$ in gradient
calculations for the real $Z$ for computational efficiency.  In our
experiments, we used S-CODE with its sampling based stochastic
gradient ascent algorithm and smoothly decreasing learning rate.

%% \subsection{S-Code with More than Two Variables}

%% In order to accommodate multiple feature types the CODE model needs to
%% be extended to handle more than two variables.
%% \cite{globerson2007euclidean} suggest the following likelihood
%% function:

%% \begin{eqnarray}
%% &\ell(\phi,& \psi^{(1)}, \ldots, \psi^{(K)}) = \label{eq:multicode}\\
%% &&\sum_k w_k \sum_{x,y^{(k)}} \bar{p}(x,y^{(k)}) \log p(x,y^{(k)}) \nonumber
%% \end{eqnarray}

%% \noindent where $Y^{(1)}, \ldots, Y^{(K)}$ are $K$ different variables
%% whose empirical joint distributions with $X$,
%% $\bar{p}(x,y^{(1)})\ldots\bar{p}(x,y^{(K)})$, are known.
%% Eq.~\ref{eq:multicode} then represents a set of CODE models
%% $p(x,y^{(k)})$ where each $Y^{(k)}$ has an embedding $\psi_y^{(k)}$
%% but all models share the same $\phi_x$ embedding.  The weights $w_k$
%% reflect the relative importance of each $Y^{(k)}$.

%% We adopt this likelihood function, set all $w_k=1$, let $X$
%% represent a word, $Y^{(1)}$ represent a random substitute and
%% $Y^{(2)}, \ldots, Y^{(K)}$ stand for various morphological and
%% orthographic features of the word.  With this setup, the training
%% procedure needs to change little: each time a word --
%% random-substitute pair is sampled, the relevant word -- feature pairs
%% are also generated and input to the gradient ascent algorithm.

%% S-CODE handles two variables, whereas underlying syntactic categories
%% can be captured by more than two different variables such as
%% contextual, morphologic and ortographic features.  S-CODE can be
%% extented to handle more than two variables in a way similar to the
%% multi variable extension of CODE \cite{globerson2007euclidean} with
%% the unit sphere restriction.  The log-likelihood at
%% Equation~\ref{eq:likelihood} can be redefined for $n+1$ different
%% categorical variables $X$, $Y_i$, $\hdots$ and $Y_n$ with finite
%% cardinalities $|X|$, $|Y_1|$, $\hdots$ and $|Y_n|$, respectively, as:
%% \begin{eqnarray}
%% &&\ell(\phi, \psi_1,\hdots,\psi_n) = \sum_{i=1}^n\sum_{x,y_i} \bar{p}(x,y_i) \log p(x,y_i) \label{eq:multiscode} \\
%% &&= \sum_{i=1}^n\sum_{x,y_i} \bar{p}(x,y_i) (-\log Z_i + \log \bar{p}(x)\bar{p}(y_i) - d^2_{x,y_i}) \nonumber \\
%% &&=-\sum_{i=1}^n(\log Z_i + \mathit{const}_i - \sum_{x,y_i} \bar{p}(x,y_i) d^2_{x,y_i}) \nonumber
%% \end{eqnarray}
%% where $\psi_i$ is the embedding of $y_i \in Y_i$ and $Z_i$ is the
%% normalization term of $p(x,y_i)$.  Thus the model is able to jointly
%% learn the embeddings when the pairwise co-occurence statistics,
%% $\bar{p}(x,y_i)$, are available for all $i$.

%% One problem with these setting is, not every $(x,y_i)$ pair is
%% observed in the data.  For example, the stem word ``\textbf{car}''
%% doesn't have any morphological feature, thus its morphological feature
%% is represented by a null value, ``X''.  However setting the unobserved
%% features to ``X'' leads to pulling the words with unobserved features
%% together even they are from different clusters or pushing the ones
%% with observed features apart even they are from same clusters.  To
%% solve this, during the gradient search we don't perform any pull or
%% push updates on embeddings if the value of $y_i$ is set to null.
