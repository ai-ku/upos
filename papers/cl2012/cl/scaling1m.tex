\subsection{WSJ 1M Results}
\label{sec:wsj}

% what is the test data

In this subsection, we expanded our test corpus from PTB24K to PTB and
applied spectral clustering with 45 clusters.
%
New language model is trained with the same settings in
Section~\ref{sec:subapp}, with exception of observation threshold for
words.  Threshold is dropped to 20 and vocabulary size increases to
78,498.
%
The perplexity of the 4-gram language model on the test corpus is 96.

Only the top 100 substitutes and their unnormalized probabilities were
computed for each of the 1,173,766 positions in the test set since low
probability substitutes are relatively unimportant, as resulted in
Section~\ref{sec:dist}, and computational efficiency is a serious
concern with the data\footnote{The substitutes with unnormalized log
  probabilities can be downloaded from
  \mbox{\url{http://goo.gl/jzKH0}}.  For a description of the {\sc
    fastsubs} algorithm used to generate the substitutes please see
  \mbox{\url{http://arxiv.org/abs/1205.5407v1}}.  {\sc fastsubs}
  accomplishes this task in about 5 hours, a naive algorithm that
  looks at the whole vocabulary would take more than 6 days on a
  typical 2012 workstation.}.  The probability vectors for each
position were normalized to add up to 1.0 giving us the final
substitute vectors used in the rest of this study.

Supervised baseline scores recomputed for distance metrics, with
absence of KL2, which is undefined in the sparse setting, and log
space distances, which previously performed poorly
(see Appendix~\ref{app:dist1M}). Score ordering found to be in line with
results from Section~\ref{sec:dist} and KL2's successor Manhattan
metric is chosen for further computations.

% here's some comments on results of spectral clustering.

Spectral clustering on the PTB scored .58??\mto accuracy.  To
enforce one-tag-per-word assumption we collapsed the result of
spectral clustering and assigned word instances to their most observed
cluster by which \mto accuracy is increased to .68??.
