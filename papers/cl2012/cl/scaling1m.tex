% what is the test data
In this section, we expanded our test corpus to the complete Wall
Street Journal Section of the Penn Treebank \cite{treebank3}
(1,173,766 tokens, 49,206 types) and reperformed spectral clustering
with 45 clusters.
%
Words that were observed less than 20 times in the language model
training data were replaced by \textsc{unk} tags, which gave us a
vocabulary size of 78,498.
%
The perplexity of the 4-gram language model on the test corpus is 96.

Due to relative unimportance of low probability substitutes, resulted
in section~\ref{sec:dist}, and computational efficiency concerns only
the top 100 substitutes and their unnormalized probabilities were
computed for each of the 1,173,766 positions in the test
set\footnote{The substitutes with unnormalized log probabilities can
  be downloaded from \mbox{\url{http://goo.gl/jzKH0}}.  For a
  description of the {\sc fastsubs} algorithm used to generate the
  substitutes please see
  \mbox{\url{http://arxiv.org/abs/1205.5407v1}}.  {\sc fastsubs}
  accomplishes this task in about 5 hours, a naive algorithm that
  looks at the whole vocabulary would take more than 6 days on a
  typical 2012 workstation.}.  The probability vectors for each
position were normalized to add up to 1.0 giving us the final
substitute vectors used in the rest of this study.

Supervised baseline scores again computed for distance metrics, with
absence of KL2, which is undefined in the sparse setting, and log
space distances, which previously performed poorly (REFERANCE TO
APPENDIX HERE). Score ordering found to be in line with results from
section~\ref{sec:dist} and Manhattan metric is chosen to replace KL2
metric.

% here's some comments on results of spectral clustering.

