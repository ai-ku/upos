\section{Experiments}
\label{sec:multilang}
\noindent We perform experiments with a range of languages and three
different feature configurations to establish both the robustness of
our model across languages and to observe the effects of different
features.  Following Christodoulopoulos et
al. \shortcite{christodoulopoulos-goldwater-steedman:2011:EMNLP}, in
addition to the PTB we extend our experiments to 8 languages from
MULTEXT-East (Bulgarian, Czech, English, Estonian, Hungarian,
Romanian, Slovene and Serbian) \cite{citeulike:5820223} and 10
languages from the CoNLL-X shared task (Bulgarian, Czech, Danish,
Dutch, German, Portuguese, Slovene, Spanish, Swedish and Turkish)
\cite{Buchholz:2006:CST:1596276.1596305}.  For all experiments, we use
the best performing model of Section~\ref{sec:code} (i.e. the random
substitute model) with default settings.  To make meaningful
comparison with the previous work we only use the training section of
MULTEXT-East\footnote{Languages of MULTEXT-East corpora do not tag the
  punctuation marks, thus we add an extra tag for punctuation to the
  tag-set of these languages.} and CONLL-X languages
\cite{Lee:2010:STU:1870658.1870741}.  

Section~\ref{sec:multivecfeat} details the language model and feature
statistics of each language.  Section~\ref{sec:multires} summarizes
the results of our models for all of the languages in our test
corpora.

\subsection{Substitute Vectors and Features}
\label{sec:multivecfeat}
To calculate the top 100 substitutes of each position, we train a
4-gram language model with the corresponding training corpora of each
language as described in Section~\ref{sec:expset}.
Table~\ref{tab:lmstatistics} presents statistics related to the
language model training and testing corpora.  For all languages except
Serbian, English and Turkish, we train the language models by using
the corresponding Wikipedia dump files\footnote{Latest Wikipedia dump
  files are freely available at \url{http://dumps.wikimedia.org/} and
  the text in the dump files can be extracted using WP2TXT
  (\url{http://wp2txt.rubyforge.org/})}[Should we talk about
  tokenizer??].  

\input{datatable.tex} 

Serbian shares common basis with Croatian and Bosnian therefore we
train 3 different language models using Wikipedia dump files of
Serbian together with these two languages and measure the perplexities
on Serbian test corpus.  We choose the Croatian language model since
it achieves the lowest perplexity score and unknown word ratio on
Serbian test corpus.

To train statistical language model of English, we use Wall Street
Journal data (1987-1994) extracted from CSR-III Text \cite{csr3text}
(excluding sections of the PTB) and for the Turkish language modeling
we use the web corpus collected from Turkish news and blog sites
\cite{sak2008turkish}.

Language model training files vary across the languages, in order to
reduce the unknown word ratio of resource poor languages and to
standardize the process we set the unknown word threshold to 2 for all
languages except English.  English has relatively low unknown word
ratio therefore we set the threshold to 20 instead of 2. [??double
  check for Multext-East English.??]

%\subsection{Features}
We use the same set of orthographic features described in
Section~\ref{sec:feat} except we add an ``Only-Punctuation'' feature
to the languages of MULTEXT-East corpora.  The ``Only-Punctuation''
feature is generated when a token consists of punctuations.

Morphological features are extracted by the method described in
Section~\ref{sec:feat} using the test corpus of each
language.\footnote{We don't use the language model corpora to extract
  morphological features.}  Language specific morphological feature
statistics are summarized in Table~\ref{tab:lmstatistics}.
%% \begin{table}[h]
%% \small
%% \caption{Number of induced suffix parts and word types with these suffix parts after the morfological feature extraction.}
%% \begin{tabular}{|l|l|c|c|}
%%         \hline
%%         & Language & \specialcell{Word types} & \specialcell{Suffix Parts}\\
%%         \hline
%%         \multirow{1}{*}{\begin{sideways}\textbf{WSJ}\end{sideways}} 
%%         & English & 19223 & 5575\\
%%         & & &\\\hline
%%         \multirow{8}{*}{\begin{sideways}\textbf{MULTEXT-East}\end{sideways}}
%%         & Bulgarian & 4209 & 609\\
%%         & Czech & 12848 & 2787\\
%%         & English & 4783 & 1251\\
%%         & Estonian & 13638 & 4448\\
%%         & Hungarian & 15995 & 5423\\
%%         & Romanian & 9445 & 2064\\
%%         & Slovene & 11834 & 2093\\
%%         & Serbian & 12476 & 2722\\
%%         \hline % Conll06 data
%%         \multirow{10}{*}{\begin{sideways}\textbf{CoNLL-X Shared Task}\end{sideways}}
%%         & Bulgarian & 8225&926\\
%%         & Czech & 85673 & 12443\\
%%         & Danish & 10897 & 3708\\
%%         & Dutch & 13407 & 5250\\
%%         & German & 45414 & 15219\\
%%         & Portuguese & 15721 & 5033\\
%%         & Slovene & 4781 & 1257\\
%%         & Spanish & 9316 & 2648\\
%%         & Swedish & 12725 & 3897\\
%%         & Turkish & 14227 & 5651\\
%%         \hline
%% \end{tabular}
%% \label{tab:morpho}
%% \end{table}

\subsection{Results}
\label{sec:multires}

For each language we report results: (1) without features (uPos), (2)
with orthographic features (uPos+O) and (3) with both orthographic and
morphological features (uPos+O+M).  In accord with the results of
Section~\ref{sec:wordsub}, we use the 25 dimensional sphere with 64
substitutes for all languages.  For each languge number of induced
clusters is set to number of tags in the gold-set as presented in
Table~\ref{tab:multiresults}.

As a baseline model we chose the bigram version of S-Code described in
Section~\ref{sec:bigram} which is a very strong baseline compared to
the ones used in
\cite{christodoulopoulos-goldwater-steedman:2011:EMNLP}.
Table~\ref{tab:multiresults} summarizes the \mto and \vm scores of our
models together with the bi-gram baseline and the best published
accuracies on each language corpus.
\input{multitable.tex} 

uPos significantly outperforms the bi-gram baseline in both \mto and
\vm scores on 14 languages while bigram model performs better on
Romanian, Serbian and Czech (CONLL-X).  uPos+O+M has the
state-of-the-art \mto and \vm accuracy on the PTB.  uPos+O and
uPos+O+M achieve the highest \mto scores on all languages of
MULTEXT-East corpora while scoring the highest \vm accuracies on
English and Romanian.  On the CoNLL-X languages uPos+O and uPos+O+M
models perform better than the best published \mto score on 7
languages (Czech, Dutch, German, Portuguese, Slovene, Swedish and
Turkish).  Similarly, these models achieves the top \vm scores on 7
languages (Czech, Danish, Dutch, German, Portuguese, Spanish and
Turkish).  uPos achieves the best published \mto score on CONLL-X
Bulgarian corpora.

