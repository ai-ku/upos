\section{Experiments}
\label{sec:multilang}
\noindent We perform experiments with a range of languages and three
different feature configurations to establish both the robustness of
our model across languages and to observe the effects of different
features.  Following Christodoulopoulos et
al. \shortcite{christodoulopoulos-goldwater-steedman:2011:EMNLP}, in
addition to the PTB we extend our experiments to 8 languages from
MULTEXT-East (Bulgarian, Czech, English, Estonian, Hungarian,
Romanian, Slovene and Serbian) \cite{citeulike:5820223} and 10
languages from the CoNLL-X shared task (Bulgarian, Czech, Danish,
Dutch, German, Portuguese, Slovene, Spanish, Swedish and Turkish).
\cite{Buchholz:2006:CST:1596276.1596305}.  For all experiments, we use
the best performing model of Section~\ref{sec:code} (i.e. the random
substitute model) with default settings.  To make meaningful
comparison with the previous work we only use the training section of
MULTEXT-East\footnote{Languages of MULTEXT-East corpora do not tag the
  punctuation marks, thus we add an extra tag for punctuation to the
  tag-set of these languages.} and CONLL-X languages
\cite{Lee:2010:STU:1870658.1870741}.  The number of word types and
clusters of each language are summarized in
Table~\ref{tab:multiresults}.

\subsection{Substitute Vectors and Features}

To calculate the top 100 substitutes of each position, we trained a 4-gram
language model with the corresponding training corpora of each
language as described in Section~\ref{sec:expset}.
Table~\ref{tab:lmstatistics} presents statistics related to the
language model training and testing corpora.  For all languages except
Serbian, English and Turkish, we trained the language models by using
the corresponding Wikipedia dump files\footnote{Latest Wikipedia dump
  files are freely available at \url{http://dumps.wikimedia.org/} and
  the text in the dump files can be extracted using WP2TXT
  (\url{http://wp2txt.rubyforge.org/})}[Should we talk about
  tokenizer??].
\begin{table}[ht]
%        \tiny  
  \caption{Summary of language model training and test corpora
  statistics for each language in the test set.} 
  \begin{tabular}{@{ }l@{ }|@{ }l@{ }|@{ }c@{ }|@{ }c@{ }|@{ }c@{ }|c@{ }|@{ }c@{ }|@{ }c@{ }|@{ }c@{ }|}
  \hline
    & & \multicolumn{3}{c|}{Language Model} & \multicolumn{4}{c|}{Test set}\\    \hline
    & Language & Source & \specialcell{Sentence\\Count} & \specialcell{Word\\Count} & \specialcell{Sentence\\Count} & \specialcell{Word\\Count} & \specialcell{Perplexity\\(ppl)} & \specialcell{Unknown\\Word} \\ \cline{1-9}
    \multirow{1}{*}{\begin{sideways}\textbf{WSJ}\end{sideways}} 
    &English & News & 5,187,874 & 126,170,376 & 49,208 & 1,173,766 & 79.926 & 0.012\\
    & & & & && & &\\\hline
    \multirow{8}{*}{\begin{sideways}\textbf{MULTEXT-East}\end{sideways}}
    &Bulgarian& Wikipedia &1,596,399 & 32,511,616  & 6,682 & 101,173 & 655.202 & .0565\\
    &Czech & Wikipedia &3,059,678 & 59,698,049 & 6,752 & 100,368 & 1,069.67 & .0299\\
    &English & News & 5,187,874 & 126,170,376 & 6,737 & 118,424 & 265.246 & .0288\\
    &Estonian & Wikipedia &833,677 & 14,513,571 & 6,478 & 94,898 & 871.765 & .0654\\
    &Hungarian & Wikipedia &3,250,267& 66,069,788 & 6,768 & 98,426 & 742.676 & .0449\\
    &Romanian & Wikipedia &3,250,267&66,069,788  & 6,520 & 118,328 & 666.855 & .1074\\
    &Slovene & Wikipedia & 899,329&18,969,846 & 6,689 & 112,278 & 658.711 & .0389\\
    &Serbian & Wikipedia & 782,278 & 17,129,679 & 6,677 & 108,809 & 804.962 & .0580\\
    \hline % Conll06 data
    \multirow{10}{*}{\begin{sideways}\textbf{CoNLL-X Shared Task}\end{sideways}}
    &Bulgarian& Wikipedia &1,596,399 & 32,511,616  & 12,823 & 190,217 & 538.972 & .0430\\
    &Czech & Wikipedia &3,059,678 & 59,698,049 & 72,703 & 1,249,408 & 1,233.95 &.0250\\
    &Danish & Wikipedia &1,672,003 & 35,863,945 & 5,190 & 94,386 & 351.24 & .0393\\
    &Dutch & Wikipedia &8,266,922 & 159,978,524 & 13,349 & 195,069 & 390.818 & .0476\\
    &German & Wikipedia &22,454,543&437,777,863 & 39,216 & 699,610 & 680.036 & .0487\\
    &Portuguese & Wikipedia & 5,706,037 & 150,099,154 & 9071 & 206,678 & 378.656 & .0861\\
    &Slovene & Wikipedia & 899,329 & 18,969,846 & 1,534 & 28,750 & 663.053 & .0414\\
    &Spanish & Wikipedia &11,534,351 & 332,311,650& 3,306 & 89,334 & 274.418 & .0424\\
    &Swedish & Wikipedia &1,953,794 & 32,004,538& 11,042 & 191,467 & 1,233.95 & .0250\\
    &Turkish & Web &39,595,781 & 491,195,991& 4,997 & 47,605 & 868.829 & .0508\\
    \hline
  \end{tabular}
  \label{tab:lmstatistics}
\end{table}

Serbian shares common basis with Croatian and Bosnian therefore we
trained 3 different language models using Wikipedia dump files of
Serbian together with these two languages and measured the
perplexities on Serbian test corpus.  We chose the Croatian language
model since it achieved the lowest perplexity score and unknown word
ratio on Serbian test corpus.

To train statistical language model of English, we used Wall Street
Journal data (1987-1994) extracted from CSR-III Text \cite{csr3text}
(excluding WSJ Section 00) and for the Turkish language modeling we
used the web corpus that was collected from Turkish news and blog
sites \cite{sak2008turkish}.

Language model training files vary across the languages, in order to
reduce the unknown word ratio of resource poor languages and to
standardize the process we set the unknown word threshold to 2 for all
languages except English.  English has relatively low unknown word
ratio therefore we set the threshold to 20 instead of 2. [??double
  check for Multext-East English.??]

%\subsection{Features}
We use the same set of orthographic features described in
Section~\ref{sec:feat} except we add an ``Only-Punctuation'' feature
to the languages of MULTEXT-East corpora.  The ``Only-Punctuation''
feature is generated when a token consists of punctuations.

Morphological feature of each language is extracted by the method
described in Section~\ref{sec:feat}.  Language specific morphological
feature statistics are summarized in Table~\ref{tab:morpho}.
\begin{table}[h]
\caption{Number of induced suffix parts and word types with these suffix parts after the morfological feature extraction.}
\begin{tabular}{|l|l|c|c|}
        \hline
        & Language & \specialcell{Word types} & \specialcell{Suffix Parts}\\
        \hline
        \multirow{1}{*}{\begin{sideways}\textbf{WSJ}\end{sideways}} 
        & English & 19223 & 5575\\
        & & &\\\hline
        \multirow{8}{*}{\begin{sideways}\textbf{MULTEXT-East}\end{sideways}}
        & Bulgarian & 4209 & 609\\
        & Czech & 12848 & 2787\\
        & English & 4783 & 1251\\
        & Estonian & 13638 & 4448\\
        & Hungarian & 15995 & 5423\\
        & Romanian & 9445 & 2064\\
        & Slovene & 11834 & 2093\\
        & Serbian & 12476 & 2722\\
        \hline % Conll06 data
        \multirow{10}{*}{\begin{sideways}\textbf{CoNLL-X Shared Task}\end{sideways}}
        & Bulgarian & 8225&926\\
        & Czech & 85673 & 12443\\
        & Danish & 10897 & 3708\\
        & Dutch & 13407 & 5250\\
        & German & 45414 & 15219\\
        & Portuguese & 15721 & 5033\\
        & Slovene & 4781 & 1257\\
        & Spanish & 9316 & 2648\\
        & Swedish & 12725 & 3897\\
        & Turkish & 14227 & 5651\\
        \hline
\end{tabular}
\label{tab:morpho}
\end{table}

\subsection{Results}
\input{multitable.tex} 
For each language we report results: (1) without features (uPos), (2)
with orthographic features (uPos+O) and (3) with both orthographic and
morphological features (uPos+O+M).  In accord with the results of
Section~\ref{sec:wordsub}, we use the 25 dimensional sphere with 64
substitutes for all languages.  As a baseline model we chose the
bi-gram version of S-Code described in Section~\ref{sec:bigram} which
is a very strong baseline compared to the ones used in
\cite{christodoulopoulos-goldwater-steedman:2011:EMNLP}.
Table~\ref{tab:multiresults} summarizes the \mto and \vm scores of our
models together with the bi-gram baseline and the best published
accuracies on each language corpus.

uPos significantly outperformed the bi-gram baseline in both \mto and
\vm scores on 14 languages while bi-gram model performed better on
Romanian, Serbian and Czech (CONLL-X).  uPos+O+M has the
state-of-the-art \mto and \vm accuracy on the PTB.  uPos+O and
uPos+O+M achieved the highest \mto scores on all languages of
MULTEXT-East corpora while scoring the highest \vm accuracies on
English and Romanian.  On the CoNLL-X languages uPos+O and uPos+O+M
models perform better than the best published \mto score on 7
languages (Czech, Dutch, German, Portuguese, Slovene, Swedish and
Turkish).  Similarly, these models achieved the top \vm scores on 7
languages (Czech, Danish, Dutch, German, Portuguese, Spanish and
Turkish).  uPos achieves the best published \mto score on CONLL-X
Bulgarian corpora.

