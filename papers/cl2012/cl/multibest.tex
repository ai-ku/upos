\subsection{Multilingual Experiments}
\label{sec:multilang}
\noindent We performed experiments with a range of languages and three
different feature configurations to establish both the robustness of
our model across languages and to observe the effects of different
features.  Following Christodoulopoulos et
al. \shortcite{christodoulopoulos-goldwater-steedman:2011:EMNLP}, in
addition to the PTB we extend our experiments to 8 languages from
MULTEXT-East (Bulgarian, Czech, English, Estonian, Hungarian,
Romanian, Slovene and Serbian) \cite{citeulike:5820223} and 10
languages from the CoNLL-X shared task (Bulgarian, Czech, Danish,
Dutch, German, Portuguese, Slovene, Spanish, Swedish and Turkish)
\cite{Buchholz:2006:CST:1596276.1596305}.  For all experiments, we use
the best performing model of Section~\ref{sec:clustering-w} (i.e.
clustering the word embeddings) with default settings.  To perform
meaningful comparisons with the previous work we train and evaluate
our models on the training section of MULTEXT-East\footnote{Languages
  of MULTEXT-East corpora do not tag the punctuation marks, thus we
  add an extra tag for punctuation to the tag-set of these languages.}
and CONLL-X languages \cite{Lee:2010:STU:1870658.1870741}.

Section~\ref{sec:multivecfeat} details the language model and feature
extraction for each language.  Section~\ref{sec:multires} summarizes
the results of our models for all of the languages in our corpora.  In
the rest of this section we refer to the MULTEXT-East and CONLL-X
corpora as the testing corpora and the language model training corpora
as the training corpora.

\subsubsection{Random Substitutes and Features}
\label{sec:multivecfeat}

To sample substitutes we calculate the probabilities of the top 100
substitutes for each position, we train a 4-gram language model with
the corresponding training corpora of each language as described in
Section~\ref{sec:expset}.  Table~\ref{tab:lmstatistics} presents
statistics related to the language model training and testing corpora.
For all languages except Serbian, English and Turkish, we train the
language models by using the corresponding Wikipedia dump
files\footnote{Latest Wikipedia dump files are freely available at
  \url{http://dumps.wikimedia.org/} and the text in the dump files can
  be extracted using WP2TXT (\url{http://wp2txt.rubyforge.org/})}.

Serbian shares a common basis with Croatian and Bosnian therefore we
trained 3 different language models using Wikipedia dump files of
Serbian together with these two languages and measured the
perplexities on the MULTEXT-East Serbian corpus.  We chose the
Croatian language model since it achieved the lowest perplexity score
and unknown word ratio on the MULTEXT-East Serbian corpus.

To train the statistical language model of English, we use Wall Street
Journal data (1987-1994) extracted from CSR-III Text \cite{csr3text}
(excluding sections of the PTB) and for the Turkish language modeling
we use the web corpus collected from Turkish news and blog sites
\cite{sak2008turkish}.

Language model training files vary across the languages in terms of
quality and quantity.  In order to reduce the unknown word ratio of
resource poor languages and to standardize the process we set the
vocabulary threshold to 2 for all languages except English.  English
has a relatively low unknown word ratio therefore we set the threshold
to 20 instead of 2.

% Features paragraphs
We use the same set of orthographic features described in
Section~\ref{sec:feat} except we add an ``Only-Punctuation'' feature
to the languages of MULTEXT-East corpora.  The ``Only-Punctuation''
feature is generated when a token only consists of punctuation
characters.

Morphological features are extracted by the method described in
Section~\ref{sec:feat} using the training sections of each language in
MULTEXT-East and CoNLL-X corpora\footnote{We don't use the language
  model corpora to extract morphological features.}.  Language
specific morphological feature statistics are summarized in
Appendix~D.
%Table~\ref{tab:lmstatistics}.

%% \begin{table}[h]
%% \small
%% \caption{Number of induced suffix parts and word types with these suffix parts after the morfological feature extraction.}
%% \begin{tabular}{|l|l|c|c|}
%%         \hline
%%         & Language & \specialcell{Word types} & \specialcell{Suffix Parts}\\
%%         \hline
%%         \multirow{1}{*}{\begin{sideways}\textbf{WSJ}\end{sideways}} 
%%         & English & 19223 & 5575\\
%%         & & &\\\hline
%%         \multirow{8}{*}{\begin{sideways}\textbf{MULTEXT-East}\end{sideways}}
%%         & Bulgarian & 4209 & 609\\
%%         & Czech & 12848 & 2787\\
%%         & English & 4783 & 1251\\
%%         & Estonian & 13638 & 4448\\
%%         & Hungarian & 15995 & 5423\\
%%         & Romanian & 9445 & 2064\\
%%         & Slovene & 11834 & 2093\\
%%         & Serbian & 12476 & 2722\\
%%         \hline % Conll06 data
%%         \multirow{10}{*}{\begin{sideways}\textbf{CoNLL-X Shared Task}\end{sideways}}
%%         & Bulgarian & 8225&926\\
%%         & Czech & 85673 & 12443\\
%%         & Danish & 10897 & 3708\\
%%         & Dutch & 13407 & 5250\\
%%         & German & 45414 & 15219\\
%%         & Portuguese & 15721 & 5033\\
%%         & Slovene & 4781 & 1257\\
%%         & Spanish & 9316 & 2648\\
%%         & Swedish & 12725 & 3897\\
%%         & Turkish & 14227 & 5651\\
%%         \hline
%% \end{tabular}
%% \label{tab:morpho}
%% \end{table}
\input{multitable.tex}
\input{tokentable.tex}
\subsubsection{Results}
\label{sec:multires}

For each language we report results of three models: (1) word
embeddings ($W$), (2) word embeddings with orthographic features
($W+O$) and (3) word embeddings with both orthographic and
morphological features ($W+O+M$).  Similar to the settings used in
Section~\ref{sec:clustering-w}, we use the 25 dimensional sphere with 64
substitutes for all languages.  For each language the number of
induced clusters is set to the number of tags in the gold-set as
presented in Table~\ref{tab:multiresults}.

As a baseline model we chose the syntagmatic bigram version of S-CODE
described in Section~\ref{sec:bigram} which is a very strong baseline
compared to the ones used in
\cite{christodoulopoulos-goldwater-steedman:2011:EMNLP}.
Table~\ref{tab:multiresults} summarizes the \mto\ and \vm\ scores of
our models together with the syntagmatic bigram baseline and the best
published accuracies on each language corpus.

$W$ significantly outperforms the syntagmatic bigram baseline in both
\mto\ and \vm\ scores on 14 languages.  $W+O+M$ has the
state-of-the-art \mto\ and \vm\ accuracy on the PTB.  $W+O$ and
$W+O+M$ achieve the highest \mto\ scores on all languages of
MULTEXT-East corpora while scoring the highest \vm\ accuracies on
English and Romanian.  On the CoNLL-X languages our models perform
better than the best published \mto\ or \vm\ accuracies on 10
languages.

