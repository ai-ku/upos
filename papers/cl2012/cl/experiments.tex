%\subsection{S-Code and Substitute Vector Application}
\subsection{Experimental Settings}\label{sec:expset}

To make a meaningful comparison on the PTB we ran all the experiments
using the following default settings (unless otherwise stated): (i)
each word was kept with its original capitalization, (ii) the learning
rate parameters were set to $\varphi_0=50$, $\eta_0=0.2$ for faster
convergence in log likelihood, (iii) the number of S-CODE iterations
were set to 50 million, (iv) the S-CODE dimensions and $Z$ were set to
25 and 0.166, respectively, (v) a modified k-means algorithm with
smart initialization was used \cite{arthur2007k}, and (vi) the number
of k-means restarts were set to 128 to improve clustering and reduce
variance.

Section~\ref{sec:dist} shows that low probability substitutes are
relatively unimportant thus for computational efficiency only the top
100 substitutes and their unnormalized probabilities were computed for
each positions in the PTB (1,173,766 tokens, 49,206 types) using the
{\sc fastsubs} algorithm \cite{yuret2021fastsub}\footnote{The
  substitutes with unnormalized log probabilities can be downloaded
  from \mbox{\url{http://goo.gl/jzKH0}}.}.  The probability vectors
for each position were normalized to add up to 1.0 giving us the final
substitute vectors used in the rest of this study.  We set the
vocabulary threshold to 20 which increases the vocabulary size to
78,498.

% \footnote{Reducing the vocabulary threshold to 20 from 500

%  increases the vocabulary size to 78,498 from 12,672. }

Each experiment was repeated 10 times with different random seeds and
the results are reported with standard errors in parentheses or error
bars in graphs.  Table~\ref{tab:results} summarizes all the results
reported in this section and the ones we cite from the literature.

\subsection{Random Partitions}\label{sec:rpart}
\begin{table*}[t] \footnotesize
\caption{Summary of results in terms of the \mto\ and \vm\ scores.
  Standard errors are given in parentheses when available.  Starred
  entries have been reported in the review paper
  \protect\cite{Christodoulopoulos:2010:TDU:1870658.1870714}.  Distributional
  models use only the identity of the target word and its context.
  The models on the right incorporate orthographic and
  morphological features.}
\begin{tabular}{|@{ }l@{ }|@{ }l@{ }|@{ }l@{ }|}
\hline
Distributional Models & \mto & \vm \\
\hline
Lamar et al. \shortcite{Lamar:2010:LCU:1870658.1870736} & .708 & -\\ %algorithm name LDC
Brown et al. \shortcite{Brown:1992:CNG:176313.176316}* & .678 & .630\\
%kcls(Och,1999) & .737 & .656\\
%\cite{goldwater-griffiths:2007:ACLMain} & .632 & .562\\
Goldwater et al. \shortcite{goldwater-griffiths:2007:ACLMain} & .632 & .562\\
Ganchev et al. \shortcite{Ganchev:2010:PRS:1859890.1859918}* & .625 & .548\\
Maron et al. \shortcite{maron2010sphere} & .688 (.0016)&-\\
Bigrams (Sec.~\ref{sec:bigram}) & \bgmto & \bgvm \\
Partitions (Sec.~\ref{sec:rpart}) & \rpmto & \rpvm \\
Substitutes (Sec.~\ref{sec:wordsub}) & \wsmto & \wsvm \\
\hline
\end{tabular}
\begin{tabular}{|@{ }l@{ }|@{ }l@{ }|@{ }l@{ }|}
\hline
Models with Additional Features & \mto & \vm \\
\hline
Clark \shortcite{Clark:2003:CDM:1067807.1067817}* & .712 & .655 \\
Christodoulopoulos et al. \shortcite{christodoulopoulos-goldwater-steedman:2011:EMNLP} & .728 & .661\\
Berg-Kirkpatrick et al. \shortcite{bergkirkpatrick-klein:2010:ACL} & .755 & -\\ % Interesting in  christo paper:73.9/67.7
Christodoulopoulos et al. \shortcite{Christodoulopoulos:2010:TDU:1870658.1870714} & .761 & .688\\
Blunsom and Cohn \shortcite{blunsom-cohn:2011:ACL-HLT2011} & .775 & .697\\
Substitutes and Features (Sec.~\ref{sec:feat}) & \ftmto & \ftvm \\
& & \\
& & \\
\hline
\end{tabular}
\label{tab:results}
\end{table*}

To obtain a discrete representation of the context, the
random--partitions algorithm first designates a random subset of
substitute vectors as centroids to partition the space, and then
associates each context with the partition defined by the closest
centroid in cosine distance.  Each partition thus defined gets a
unique id, and word ($X$) -- partition-id ($Y$) pairs are given to
S-CODE as input.  The algorithm uses stochastic gradient ascent to
find the $\phi_x, \psi_y$ embeddings for word and partition-id in
these pairs on a single 25-dimensional sphere.  The algorithm cycles
through the data until we get approximately 50 million updates.  The
resulting $\phi_x$ vectors are clustered using an instance weighted
k-means algorithm and the resulting groups are compared to the correct
part of speech tags.  Using default settings with 64K random
partitions the many-to-one accuracy is \rpmto\ and the V-measure is
\rpvm.

To analyze the sensitivity of this result to our specific parameter
settings we ran a number of experiments where each parameter was
varied over a range of values.

\begin{figure}[ht] \centering
\includegraphics[width=0.5\linewidth]{plot-p.pdf}
\caption{\mto\ is not sensitive to the number of partitions used to
  discretize the substitute vector space within our experimental
  range.}
\label{plot-p}
\end{figure}

Figure~\ref{plot-p} gives results where the number of initial random
partitions is varied over a large range and shows the results to be
fairly stable across two orders of magnitude.

\begin{figure}[ht] \centering
\includegraphics[width=0.5\linewidth]{plot-d.pdf}
\caption{\mto\ falls sharply for less than 10 S-CODE dimensions, but
  more than 25 do not help.}
\label{plot-d}
\end{figure}

Figure~\ref{plot-d} shows that at least 10 embedding dimensions are
necessary to get within 1\% of the best result, but there is no
significant gain from using more than 25 dimensions.

\begin{figure}[ht] \centering
\includegraphics[width=0.5\linewidth]{plot-z.pdf}
\caption{\mto\ is fairly stable as long as the $\tilde{Z}$ constant is
  within an order of magnitude of the real $Z$ value.}
\label{plot-z}
\end{figure}

Figure~\ref{plot-z} shows that the constant $\tilde{Z}$ approximation
can be varied within two orders of magnitude without a significant
performance drop in the many-to-one score.  For uniformly distributed
points on a 25 dimensional sphere, the expected $Z\approx 0.146$.  In
the experiments where we tested we found the real $Z$ always to be in
the 0.140-0.170 range.  When the constant $\tilde{Z}$ estimate is too
small the attraction in Eq.~\ref{eq:attract} dominates the repulsion
in Eq.~\ref{eq:repulse} and all points tend to converge to the same
location.  When $\tilde{Z}$ is too high, it prevents meaningful
clusters from coalescing.
%%% I have seen the first, but the second is pure guess, need to
%%% look.  The distances seem to be decreasing on that end as well!

We find the random partition algorithm to be fairly robust to
different parameter settings and the resulting many-to-one score
significantly better than the state-of-the-art distributional models.

\subsection{Random Substitutes}\label{sec:wordsub}

Another way to use substitute vectors in a discrete setting is simply
to sample individual substitute words from them according to the
corresponding probabilities.  The random-substitutes algorithm cycles
through the test data and pairs each word with a random substitute
picked from the pre-computed substitute vectors (see
Section~\ref{sec:subthr}).  We ran the random-substitutes algorithm to
generate 76 million word ($X$) -- random-substitute ($Y$) pairs (64
substitutes for each token) as input to S-CODE.  Clustering the
resulting $\phi_x$ vectors yields a many-to-one score of \wsmto\ and a
V-measure of \wsvm.

This result is close to the previous result by the random-partition
algorithm, \rpmto, demonstrating that two very different discrete
representations of context based on paradigmatic features give
consistent results.  Figure~\ref{plot-s} illustrates that the
random-substitute result is fairly robust as long as the training
algorithm can observe more than a few random substitutes per word.

\begin{figure}[ht] \centering
\includegraphics[width=0.5\linewidth]{plot-s.pdf}
\caption{\mto\ is not sensitive to the number of random substitutes
  sampled per word token.}
\label{plot-s}
\end{figure}

\subsection{Paradigmatic vs Syntagmatic Representations of Word Context}\label{sec:bigram}

To get a direct comparison of the paradigmatic and syntagmatic context
representations we feed the adjacent word pairs (bigrams) in the
corpus into the S-CODE algorithm as $X, Y$ samples
\cite{maron2010sphere} instead of pairing each word with a
paradigmatic representation of its context.  At the end each word $w$
in the vocabulary ends up with two points on the sphere, a $\phi_w$
point representing the behavior of $w$ as the left word of a bigram
and a $\psi_w$ point representing it as the right word.  The two
vectors for $w$ are concatenated to create a 50-dimensional
representation at the end.  These 50-dimensional vectors are clustered
using the k-means algorithm.  Maron et al. \shortcite{maron2010sphere}
report many-to-one scores of .6880 (.0016) for 45 clusters and .7150
(.0060) for 50 clusters (on the PTB).  If only $\phi_w$ vectors are
clustered without concatenation we found the performance drops
significantly to about .62.  Using our default settings the bigram
model achieves \bgmto\ \mto\ and \bgvm\ \vm\ accuracies.  Both results are
significantly lower than the random partition and substitute \mto\ and
\vm\ accuracies.
