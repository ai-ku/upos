\subsection{Clustering Context Embeddings (${\bf C}$)}
\label{sec:clustering-c}

In the previous section we group word types rather than word tokens by
clustering the word embeddings.  In this section we remove the
one-tag-per-word assumption and group word tokens according to
embeddings of their substitutes.  We generate 64 substitutes for each
word token and input them to S-CODE as word($W$) -- substitute (${\bf C}$)
pairs.  The resulting embeddings of the context (i.e., substitutes)
are clustered using the instance weighted k-means algorithm with 128
restarts.  The process yields 64 cluster-ids (for every pair generated
from word token's context) for each word token's context.  The
cluster-ids tokens are predicted by the majority cluster-id of the
corresponding pairs.  Ties for the majority are broken randomly.  The
many-to-one accuracy is \wsymto\ and the V-measure is \wsyvm\ .

In order to demonstrate the merit in the token based POS induction, we
first define the gold-tag perplexity for the word types as following:
\begin{equation} \label{eq:tag-perp}
GP(w) = 2^{H(p_w)} = 2^{-\sum_{t} p_w(t)log_2 p_w(t)}
\end{equation}
\noindent where $p_w$ is the gold POS tag distribution of the word
type $w$ and $H(p_w)$ is the entropy of the $p_w$ distribution.
Gold-tag perplexity ($GP$) is used to determine the POS ambiguity of
the word types, relating how often a word type is associated with
different POS tags in the test corpus.  A $GP$ of 1 for a word type
$w$ indicates $w$ is associated with same POS tag throughout the test
corpus, meaning the word type $w$'s POS is unambiguous.  As the $GP$
increases ambiguity of the word types increases and poses a handicap
for induction models that limits tag variety for the word types.  To
display the limitations, we split the test corpus in two subsets: word
types with $GP$ less than 1.75 and word types with $GP$ equal or
greater than 1.75.  We performed \mto\ evaluation on our induction
output and obtained the induced-tag -- gold-tag mappings. Using the
mappings obtained over the test corpus, we evaluated the accuracy in
the subsets. 

\begin{table}[h]
  \small
  \centering
  \caption{The \mto\ accuracy of ${\bf W}$, ${\bf C}$ and ${\bf W}\oplus{\bf C}$
    based models on two subsets that consist of words with $GP$ smaller
    and larger than 1.75, respectively.  The
    percentage of each subset in the test data is reported in the title
    bar.  The average $GP$ of each
    clustering over the whole corpus is reported in the last column.  Each
    score is an average of 10 random starts of our algorithm and the
    standard error of each one is reported in parenthesis while
    statistically the best \mto\ score of each column is reported in bold.  
  }
  \label{tab:bins}
  \begin{tabular}{|c|c|c|c||c|}
    \hline
    Model & \specialcell{$GP < 1.75$\\$89\%$} & \specialcell{$GP \ge 1.75$\\$11\%$} & \specialcell{$GP \ge 1$ \\ $100\%$} & Average $GP$ \\
    \hline
    \specialcell{Clustering ${\bf W}$ embeddings\\(Type based)} & {\bf .8054 (.0065)} & .4383 (.0104) & {\bf \wsmto} & 1.0 (.0)\\
    \hline
    \specialcell{Clustering ${\bf W} \oplus {\bf C}$ embeddings\\(Sparse-token based)}& .7322 (.0079) & {\bf .4671 (.0174)} & \wsxymto & 1.3406 (.0057)\\ 
    \hline
    \specialcell{Clustering ${\bf C}$ embeddings\\(Token based)} & .6620 (.0051) & .4309 (.0093) & \wsymto & 1.5318 (.0076)\\
    \hline  
  \end{tabular}
\end{table}

The performance of our algorithm on ${\bf C}$ embeddings is summarized
in Table~\ref{tab:bins}.  Due to the one-tag-per-word nature of POS
induction, the type based model outperforms the token based one on the
unambiguous words. The token based model achieves statistically
comparable results with the type based model on the ambiguous words.
Type based model can not handle words with ambiguity while the token
based model can.  In order to take advantage of both models we apply
our algorithm on concatenation of ${\bf W}$ and ${\bf C}$ embeddings
in the next section.

\subsection{Clustering Concatenation of Word and Context Embeddings (${\bf W}\oplus{\bf C}$)}
\label{sec:clustering-concatenation}

Two models presented in earlier sections perform POS induction either
by assuming (Section~\ref{sec:clustering-w}) or discarding
(Section~\ref{sec:clustering-c}) the one-tag-per-word assumption.  In
this section we define a sparse-token based model which clusters the
concatenation of ${\bf W}$ and ${\bf C}$ embeddings.  This model not
only tends to put instances of a word type into the same cluster but
also performs token based clustering by incorporating the word type
and context information together.

Similar to the previous models, we generate ${\bf W}$ -- ${\bf C}$
pairs as the input to S-CODE.  For each observed ${\bf W}$ -- ${\bf
  C}$ pair in the S-CODE input, corresponding 25-dimensional $\phi_w$
and $\psi_c$ embeddings are concatenated to create a 50-dimensional
representation.  We used the same experimental setting of the previous
section and predict the token clusters according to the majority
cluster-id of the corresponding pairs.  The many-to-one accuracy of
this model is \wsxymto\ and the V-measure is \wsxyvm\ .

Table~\ref{tab:bins} presents the performance of the ${\bf
  W}\oplus{\bf C}$ based model over the subsets and it achieves
statistically better \mto\ than both of the ${\bf W}$ and ${\bf C}$
based models on ambiguous words.  Due to the bias towards to the
sparse clustering, sparse-token based model statistically improves the
\mto\ accuracy on unambiguous words compared to the ${\bf C}$ based
model but it still can not achieve the performance of the ${\bf W}$
based model.  The ${\bf W}\oplus{\bf C}$ based model constructs token
based clusters that tend to assign instances of a word type into the
same clusters which leads to a smaller average $GP$ than the ${\bf C}$
based model as shown in Table~\ref{tab:bins}.

%% We don't really need this part
%% \subsubsection{Paradigmatic vs Syntagmatic Representations of Word Context}
%% \label{sec:bigram-token}
%% In order to compare the token clustering performance of the
%% paradigmatic and the syntagmatic context representations we use the
%% same 4 models defined in Section~\ref{sec:bigram-type}.  Following the
%% previous section we concatenate the 25-dimensional $\phi_x$ and
%% $\psi_y$ ($\psi_{y_{1}}$ and $\psi_{y_{2}}$ in the fourth model)
%% embeddings of the corresponding observed pairs (tuples in the fourth
%% model) and represent the first three models outputs with a
%% 50-dimensional vectors (75-dimensional vectors in the fourth model).
%% The resulting vectors are clustered using k-means algorithm with 128
%% restarts.
%% \begin{table}[ht]
%% \centering
%% \small
%% \caption{Accuracies of the token based S-CODE models on the gold-tag
%%   perplexity separated subsets.}
%% \begin{tabular}{|l|l|l|l|}
%% \hline
%% Model & \specialcell{$GP < 1.75$\\$89\%$} & \specialcell{$GP \ge 1.75$\\$11\%$} & \specialcell{$GP \ge 1.0$\\$100\%$}\\
%% \hline
%% $X$ (word) - $Y$ (left bigram) & .5950 (.0051) & .4783 (.0005) & .5821 (.0041)\\
%% $X$ (word) - $Y$ (right bigram) & .6239 (.0049) & .3075 (.0153) & .5891 (.0046)\\
%% $X$ (word) - $Y$ (left and right bigram concatenation) & .7523 (.0065) & .4492 (.0240) & .7190 (.0049)\\
%% $X$ (word) - $Y_1$, $Y_2$ (left and right bigrams) & .6697 (.0065) & .4579 (.0052) & .6464 (.0051)\\
%% $X$ (word) - $Y$ (random substitutes) & .7322 (.0079) & .4671 (.0174) & .7030 (.0073)\\
%% \hline
%% \end{tabular}
%% \label{tab:tokens}
%% \end{table}
