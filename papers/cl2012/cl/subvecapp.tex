%%substitute vector applications
\section{Substitute Vector Applications}
\label{sec:subapp}
This section details the choice of the data set, the vocabulary and
the estimation of substitute probabilities for substitue vector
application in part-of-speech induction problem.

% what is the LM training data
%Train => 5181717 126019973 690121813
To compute substitute probabilities we trained a language model using
approximately 126 million tokens of Wall Street Journal data
(1987-1994) extracted from CSR-III Text \cite{csr3text} (excluding WSJ
Section 00).
% how is the language model trained
We used SRILM \cite{Stolcke2002} to build a 4-gram language model with
Kneser-Ney discounting.
% what is the vocabulary
Words that were observed less than 500 times in the LM training data
were replaced by \textsc{unk} tags, which gave us a vocabulary size of
12,672.
% what is the test data
The first 24,020 tokens of the Penn Treebank \cite{treebank3} Wall
Street Journal Section 00 was used as the corpus to be induced.
Corpus size kept small in order to efficiently compute full distance
matrices.  Substituion probabilities for 12,672 vocabulary words were
computed at each of the 24,020 positions, as described in
Section~\ref{sec:subthr}.
% perplexity
The perplexity of the 4-gram language model on the test corpus was
55.4 which is quite low due to using a small
vocabulary and in-domain data.
% what is the tag set
The treebank uses 45 part-of-speech tags which is the set we used as
the gold standard for comparison in our experiments.  
