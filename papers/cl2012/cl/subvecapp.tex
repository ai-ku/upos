%%substitute vector applications
\section{Substitute Vector Applications}
\label{sec:subapp}
In this section, we explored the limits and potential of using solely
substitute vectors (without using the correponding target word
identities) on the part-of-speech induction problem.  Rest of this
section details the choice of the data set, the vocabulary and the
estimation of substitute probabilities.  

% what is the LM training data
%Train => 5181717 126019973 690121813
To compute substitute probabilities we trained a language model using
approximately 126 million tokens of Wall Street Journal data
(1987-1994) extracted from CSR-III Text \cite{csr3text} (excluding WSJ
Section 00).
% how is the language model trained
We used SRILM \cite{Stolcke2002} to build a 4-gram language model with
Kneser-Ney discounting.
% what is the vocabulary
Words that were observed less than 500 times in the LM training data
were replaced by \textsc{unk} tags, which gave us a vocabulary size of
12,672.
% what is the test data
The first 24,020 tokens of the Penn Treebank \cite{treebank3} Wall
Street Journal Section 00 was used as the test corpus to be induced.
Corpus size kept small in order to efficiently compute full distance
matrices.  Substitution probabilities for 12,672 vocabulary words were
computed at each of the 24,020 positions, as described in
Section~\ref{sec:subthr}.
% perplexity
The perplexity of the 4-gram language model on the test corpus was
55.4 which is quite low due to using a small
vocabulary and in-domain data.
% what is the tag set
The treebank uses 45 part-of-speech tags which is the set we used as
the gold standard for comparison in our experiments.

Section~\ref{sec:dist} gives a detailed comparison of similarity
metrics in high dimensional substitute vector space.
Section~\ref{sec:dimreduce} analyzes application of possible
dimensionality reduction algorithms to our problem.
Section~\ref{sec:clustering} gives comparison of various clustering
techniques and finally Section~\ref{sec:wsj} applies the findings of
the previous sections to the 1M word Penn Treebank corpus.
