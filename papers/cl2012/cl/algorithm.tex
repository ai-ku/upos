\section{Algorithm}

In this section we describe the components of our algorithm and their
relationship with each other.  The algorithm predicts the syntactic
category of a word in a given context based on its random substitutes.
In other words first we construct the co-occurrence representation of
words and their substitutes with the help of a language model and then
map each value in the co-occurrence data to a corresponding embedding
on a $n$-dimensional sphere using the S-CODE algorithm
\cite{maron2010sphere}.  Finally, we apply k-means clustering to
categorize the word embeddings by which we induced the word
categories.  In the next subsection we detail the representation of
word contexts as co-occurrence data, in Subsection~\ref{sec:embedding}
we explain the embedding calculation and finally in
Subsection~\ref{sec:clustering} we describe the different ways of
embedding clustering.

\subsection{Context Representation}
\label{sec:cooc}
% How we represent the context?
% How we relate the word and the context?

Word contexts are represented by random substitutes that are sampled
from the corresponding substitute word distributions.  Random
substitutes are sampled with replacement from the substitute
distributions that are calculated based on an n-gram language model.
The sample space of the substitute word distributions is the
vocabulary of the language model.\footnote{Sampled substitutes might
  include the unknown word tag ``<unk>'' since it is in the language
  model vocabulary.  For example substitutes of proper nouns usually
  include ``<unk>'' as a substitute.}  It is possible (and beneficial)
to sample more than one substitutes and generate more pairs from the
same substitute distribution as in Table~\ref{tab:samples}.  The
calculation of substitute distributions and random substitute sampling
are detailed in Appendix~A.  To capture the relation between each word
and its context we construct a co-occurrence representation by pairing
the words with their random substitutes.  Table~\ref{tab:samples}
shows random substitutes of each word and their co-occurrence
representation on an example sentence.  A target word might appear
both as a word and a random substitute therefore to clarify this
ambiguity we concatenate ``W'' and ``C'' to words and contexts (i.e.,
random substitutes), respectively, in the co-occurrence data.

The next section explains the S-CODE algorithm which takes the
co-occurrence data as its input and calculates the embeddings of the
words and their substitutes on an $n$-dimensional sphere.  In the rest
of the paper we use the term ``substitutes'' and ``random
substitutes'' interchangeably.

\begin{table}[ht]
  \caption{The table on the left shows three possible substitutes, separated
    with ``/'', sampled for each position in the example sentence
    \textit{``Pierre Vinken, 61 years old, will join the board as a
      nonexecutive director Nov.~29 .''} based on a 4-gram language
    model.  The right one represents the input sentence as
    co-occurrences of words and their substitutes.  Thus words on the
    left column presents the target word while words on the right column
    represents the context of the corresponding target word.}
\begin{tabular}{|ll|} \hline
\textbf{Word} & \textbf{Random Substitutes}\\
\hline
Pierre & \textit{Mr.}  / \textit{Pierre} /  \textit{Mr.}\\
Vinken & \textit{<unk>} / \textit{Beregovoy} / \textit{Cardin}\\
, & \textit{,} / \textit{,} / \textit{,}\\
61 & \textit{48} / \textit{52} / \textit{41}\\
years & \textit{years} /  \textit{years} /  \textit{years}\\
old & \textit{old} /  \textit{old} /  \textit{old}\\
, & \textit{,} /  \textit{,} /  \textit{,}\\
will & \textit{will} /  \textit{will} /  \textit{will}\\
join & \textit{head} /  \textit{join} /  \textit{leave}\\
the  & \textit{its} /  \textit{its} /  \textit{the}\\
board & \textit{board} /  \textit{company} / \textit{firm}\\
as & \textit{as} / \textit{as} / \textit{as}\\
a & \textit{a} / \textit{a} / \textit{a}\\
nonexecutive & \textit{nonexecutive} / \textit{non-executive} / \textit{nonexecutive}\\
director & \textit{chairman} / \textit{chairman} / \textit{director}\\
Nov. & \textit{April} / \textit{May} / \textit{of}\\
29 & \textit{16} /  \textit{29} / \textit{9}\\
. & \textit{.}  / \textit{.} / \textit{.}\\
\hline
\end{tabular}
\quad
\begin{tabular}{|ll|}
\hline
\textbf{Word} & \textbf{Context}\\
\hline
W:Pierre & \textit{C:Mr.}\\
W:Pierre & \textit{C:Pierre}\\
W:Pierre & \textit{C:Mr.}\\
W:Vinken & \textit{C:<unk>}\\
W:Vinken & \textit{C:Beregovoy}\\
W:Vinken & \textit{C:Cardin}\\
$\hdots$&\\
W:join & \textit{C:head}\\
W:join & \textit{C:join}\\
W:join & \textit{C:leave}\\
W:the & \textit{C:its}\\
W:the & \textit{C:its}\\
W:the & \textit{C:the}\\
$\hdots$&\\
W:director & \textit{C:chairman}\\
W:director & \textit{C:chairman}\\
W:director & \textit{C:director}\\
$\hdots$&\\
\hline
\end{tabular}
\label{tab:samples}
\end{table}

\subsection{Co-occurrence Embedding}
\label{sec:embedding}
The S-CODE algorithm maps each word and substitute value in the
co-occurrence data to an embedding on an $n$-dimensional sphere as
detailed in Appendix~B.  The basic idea of the mapping is that words
and substitutes that are frequently observed as pairs in the
co-occurrence data will have close embeddings while unobserved pairs
will have embeddings that are apart from each other.
\begin{figure}[ht]
\centering
  \begin{minipage}[c]{0.38\textwidth}
    \begin{tabular}{|l|l|}
    \hline
    \textbf{Word} & \textbf{Context} \\
    \hline
    $\hdots$&$\hdots$\\
    W:director & C:chairman \\
    W:chief & C:chairman \\
    $\hdots$&$\hdots$\\
    W:Pierre & C:Mr. \\
    W:Frank & C:Mr. \\
    $\hdots$&$\hdots$\\
    \hline
  \end{tabular}
  \end{minipage}
  \begin{minipage}[c]{0.48\textwidth}
    \includegraphics[height=\textwidth]{scode-ex.png}
  \end{minipage}
  \caption{The figure on the right is the final embeddings of the
    input co-occurrence data given on the left table after S-CODE
    converges.  Dashed circles represent the possible groupings of the
    embeddings on the sphere.}
  \label{fig:scodeexample}
\end{figure}

The co-occurrence data in Figure~\ref{fig:scodeexample} consists of
pairs such as (\textit{W:director}, \textit{C:chairman}) and
(\textit{W:chief}, \textit{C:chairman}) therefore S-CODE forces the
embeddings of \textit{W:director} and \textit{W:chief} to be close to
the embedding of \textit{C:chairman}.  Similar to the former case the
embeddings of \textit{W:Pierre} and \textit{W:Frank} will be close to
the embedding \textit{C:Mr.} because of the frequently observed pairs.
As a result the final embeddings of \textit{W:director} and
\textit{W:chief} will be close to each other due to the common
substitute \textit{C:chairman} and will be apart from
\textit{W:Pierre} and \textit{W:Frank} due to the lack of common
substitute as shown on Figure~\ref{fig:scodeexample} (similarly the
embeddings of \textit{W:Pierre} and \textit{W:Frank} will be close to
each other due to \textit{C:Mr.}).

% How we relate final embeddings and the input pairs?
S-CODE constructs embeddings on $n$-dimensional sphere for each word
type and substitute.  Each pair in the co-occurrence data can be
represented in three different ways by using the output of S-CODE: (1)
word embedding (${\bf W}$) which represents the word type information,
(2) substitute embedding (${\bf C}$) which represents the context
information, and (3) concatenation of word and substitute embeddings
(${\bf W}\oplus{\bf C}$).  In the next section we apply k-means
clustering to these three representation and analyze the
characteristic of final clusters.

\subsection{Embedding Clustering}
\label{sec:clustering}
%% In Section~\ref{sec:cooc} we decribe the tranformation of an input
%% sentence to a co-occurrence data and we represent each target word
%% with the word--substitute pair(s).  In the previous section we
%% construct embeddings for each value observed in the co-occurrence data
%% using the S-CODE algorithm. 

Each target word in the original input sentence is represented with
word--substitute pairs and each unique word and substitute value are
represented with embeddings on an $n$ dimensional sphere.  Therefore
clustering the embeddings means clustering the target words in the
original input.  We run instance weighted k-means algorithm to cluster
the final embeddings constructed by S-CODE.  The target words can be
represented in three ways as suggested in the previous section and the
clustering output of each representation will have different
characteristics.  We use instance weighted k-means to cluster each
representation.

\paragraph{Word embeddings (${\bf W}$)} All tokens of a word type are
represented with the same embedding.  For instance, although we sample
three substitutes per target word in Table~\ref{tab:samples}, each
word type has only one embedding in ${\bf W}$.  Thus clustering target
words based on this representation employ the one-tag-per-word
assumption from the beginning.  The one-tag-per-word assumption is
suitable for the part of speech induction problem given that 93.69\%
of the word occurrences in the human labeled PTB data are tagged with
their most frequent part of speech
\cite{Toutanova:2003:FPT:1073445.1073478}.  However the clustering
performance on the ambiguous words (words that have more than one tag)
degrades due to the one-tag-per-word assumption.  For example, the
word {\em offer} is tagged as NN(399), VB(105) and VBP(34) in its 538
WSJ instances\footnote{NN, VB and VBP are three part-of-speech tags
  from the Penn Treebank corpus and they correspond to singular noun,
  verb in base form and non-$3^{rd}$person singular verb in present
  tense, respectively.}.  In this scheme all instances of {\em offer}
tagged with one tag and \mto\ upper bound of {\em offer} will be .7616
due to the most frequent tag NN(399). Our model in
Section~\ref{sec:clustering-w} clusters word embeddings and achieves
the \mto\ upper bound of {\em offer} however this is not the case for
all ambiguous words.

%offer tag distribution in WSJ VBP 34, VB 105, NN 399
\paragraph{Context embeddings (${\bf C}$)} 
Each target word token is represented with embeddings of its
substitutes that depends on the token context.  Models based on this
representation cluster word tokens and do not force the
one-tag-per-word assumption.  For example, the word ``Pierre'' in
Table~\ref{tab:samples} will be represented with the embeddings of
{\it S:Mr.}, {\it S:Pierre} and {\it S:Mr.}  while another occurrence
of the word ``Pierre'' in a different context might be represented
with different substitutes.  It is possible to represent the context
with several substitutes, in such a case k-means might group
substitute embeddings into different clusters which leads to an
ambiguity on the final cluster of the target word.  To solve this
issue the target word is assigned to the cluster in which the majority
of its substitute embeddings are present\footnote{Ties are broken
  randomly.}.  The model in Section~\ref{sec:clustering-c} clusters
${\bf C}$ and achieves .8215 \mto\ on the word {\em offer} which is
higher than the one-tag-per-word \mto\ upper bound.

\paragraph{Concatenation of word and context embeddings (${\bf W\oplus
    C}$)}
This representation concatenates the word type and its substitute
embeddings such that each word instance is represented with $r$
vectors where $r$ is the number of random substitutes per target word.
For instance, the target word ``Pierre'' in Table~\ref{tab:samples}
will be represented with three vectors that will be the concatenation
of the embedding of {\it W:Pierre} with the embeddings of {\it C:Mr.},
{\it C:Pierre} and {\it C:Mr.}, separately.  Models that are based on
this representation do not employ the one-tag-per-word assumption and
cluster word tokens.  Clusters that are constructed according this
representation tend to be sparser than the previous one due to the
concatenation of ${\bf W}$.  Similar to the former case we run k-means
on these concatenated vectors and use majority voting to decide the
target word cluster.  The model in
Section~\ref{sec:clustering-concatenation} clusters concatenation of
embeddings and outperforms the previous models by scoring .8866 \mto\
accuracy on the word {\em offer}.

The first representation applies the one-tag-per-word assumption from
the beginning and clusters word types instead of tokens.  On the other
hand the second one relaxes the one-tag-per-word assumption and
clusters tokens by using context embeddings.  The final one clusters
tokens by representing each token with the concatenation of the word
and context embeddings therefore it is more biased to sparse clusters
than the previous one.

Section~\ref{sec:exp} compares the performance of these three
representations on the POS induction problem.
