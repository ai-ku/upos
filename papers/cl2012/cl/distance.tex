\subsection{Distance Metric}
\label{sec:dist}

We represent each context with a sparse high dimensional probability
vector called the substitute vector as described in the previous
section.  In this section we compare various distance metrics in this
high dimensional space with the goal of discovering one that will
judge vectors that belong to the same syntactic category similar and
vectors that belong to different syntactic categories distant.  The
distance metrics we have considered are listed in
Table~\ref{tab:metrics}.

% we should remove KL2 metric from here, because we have sparse vectors
% also update the table caption

\begin{table}[ht] \centering
\small
\begin{tabular}{|lll|}
\hline
%% Pretify this table norm numbers looks weird.
Cosine($\mathbf{p}, \mathbf{q}$) & = & $<\mathbf{p},\mathbf{q}> / (\|\mathbf{p}\|_{2} \|\mathbf{q}\|_{2})$ \\
Euclid($\mathbf{p}, \mathbf{q}$) & = & $\|\mathbf{p} - \mathbf{q}\|_{2}$ \\
Manhattan($\mathbf{p}, \mathbf{q}$) & = & $\|\mathbf{p} - \mathbf{q}\|_{1}$ \\
Maximum($\mathbf{p}, \mathbf{q}$) & = & $\|\mathbf{p} - \mathbf{q}\|_{\infty}$ \\
KL2($\mathbf{p}, \mathbf{q}$) & = & $\sum_i p_iln(p_i/q_i) + q_iln(q_i/p_i) $\\
JS($\mathbf{p}, \mathbf{q}$) & = & $\sum_i p_iln(p_i/m_i) + q_iln(q_i/m_i) $\\
& & where $m_i = (p_i + q_i) / 2$\\
\hline
\end{tabular}
\caption{Similarity metrics.  JS is the Jensen-Shannon divergence and
  KL2 is a symmetric implementation of Kullback-Leibler divergence.}
\label{tab:metrics}
\end{table}


% are we still using k = 30 ? update if necessary
To judge the merit of each distance metric we obtained supervised
baseline scores using leave-one-out cross validation and the weighted
k-nearest-neighbor algorithm\footnote{Neighbors were weighted using
  1/distance, $k=30$ was chosen empirically.} on the gold tags of the
1M word WSJ corpus.  The results are listed in
Table~\ref{tab:distscores} sorted by score.


% update scores in this table
\begin{table}[ht] \centering
\begin{tabular}{|l|c|}
\hline
Metric & Accuracy(\%) \\
\hline
%% K=30 24K WSJ
KL2 & 0.6889 \\
Manhattan & 0.6865 \\
Jensen & 0.6801 \\
Cosine & 0.6706 \\
Maximum & 0.6663 \\
Euclid & 0.6255 \\
lg2-Maximum & 0.5361 \\
lg2-Cosine & 0.4847 \\
lg2-Euclid & 0.4038 \\
lg2-Manhattan & 0.3729 \\
%% K=30 1M WSJ
%% KL2 & ? \\
%% Manhattan & .7354\\ %0.735381668919
%% Jensen & .7317 \\ %0.731718247078
%% Cosine & .7240 \\ %0.724018245545
%% Maximum & ? \\
%% Euclid & .6109 \\ %0.610962491672
%% lg2-Maximum & ? \\
%% lg2-Cosine & ? \\
%% lg2-Euclid & ? \\
%% lg2-Manhattan & ? \\
\hline
\end{tabular}
\caption{Supervised baseline scores with different distance metrics.
  Log-metric indicates that metric applied to the log of the
  probability vectors.}
\label{tab:distscores}
\end{table}

% K=1 1M WSJ
% KL2 can we define kl2 on sparse vectors
% Manhattan 
% Jensen 0.684500999347
% Cosine 0.672916918704
% Maximum 
% Euclid 0.597443613122
% lg2-Maximum 
% lg2-Cosine 
% lg2-Euclid 
% lg2-Manhattan 

% K=20 1M WSJ
% KL2 & ?\\
% Manhattan & ?\\
% Jensen & 0.732678404384\\
% Cosine & 0.720252797472\\
% Maximum & ?\\
% Euclid & 0.617086369856\\
% log-Maximum & ?\\
% log-Cosine & ?\\
% log-Euclid & ?\\
% log-Manhattan & ?\\


% update scores in this paragraph as well
The entries with the log- prefix indicate a metric applied to the log
of the probability vectors.  Distance metrics on log probability
vectors performed poorly compared to their regular counterparts
indicating differences in low probability words are relatively
unimportant and high probability substitutes determine syntactic
category.  The surprisingly good result achieved by the simple Maximum
metric (which identifies the dimension with the largest difference
between two vectors) also support this conclusion.  The maximum score
of .73\% can be taken as a rough upper bound for an unsupervised
learner using this space on the 45-tag 1M word WSJ corpus because
.27\% of the instances are assigned to the wrong part of speech by the
majority of their closest neighbors.  We will discuss ways to push
this upper bound higher by including word type information together
with other features in Section~\ref{sec:scode} and
Section~\ref{sec:features}, respectively.

% moreover we are using probability vectors so its more natural to use
% kl2

%%%% Distance calcualtion on sparse 24K vectors
% K=30
% KL2 & ?\\
% Manhattan & 0.694629475437\\
% Jensen & 0.684554537885\\
% Cosine & 0.668068276436\\
% Maximum & 0.663821815154\\
% Euclid & 0.639841798501\\
% log-Maximum & ?\\
% log-Cosine & ?\\
% log-Euclid & ?\\
% log-Manhattan & ?\\
% Distance files are located at:
% /work/upos_alllang/run.mali/test.wsj24k.knn[0,1,2,3,4].gz
