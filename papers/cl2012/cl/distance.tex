\section{Distance Metric}
\label{sec:dist}

We represent each context with a high dimensional probability vector
called the substitute vector as described in the previous section.  In
this section we compare various distance metrics in this high
dimensional space with the goal of discovering one that will judge
vectors that belong to the same syntactic category similar and vectors
that belong to different syntactic categories distant.  The distance
metrics we have considered are listed in Table~\ref{tab:metrics}.

\begin{table}[ht] \centering
\small
\begin{tabular}{|lll|}
\hline
Cosine($\mathbf{p}, \mathbf{q}$) & = & $<\mathbf{p},\mathbf{q}> / (\|\mathbf{p}\|_{2} \|\mathbf{q}\|_{2})$ \\
Euclid($\mathbf{p}, \mathbf{q}$) & = & $\|\mathbf{p} - \mathbf{q}\|_{2}$ \\
Manhattan($\mathbf{p}, \mathbf{q}$) & = & $\|\mathbf{p} - \mathbf{q}\|_{1}$ \\
Maximum($\mathbf{p}, \mathbf{q}$) & = & $\|\mathbf{p} - \mathbf{q}\|_{\infty}$ \\
KL2($\mathbf{p}, \mathbf{q}$) & = & $\sum_i p_iln(p_i/q_i) + q_iln(q_i/p_i) $\\
JS($\mathbf{p}, \mathbf{q}$) & = & $\sum_i p_iln(p_i/m_i) + q_iln(q_i/m_i) $\\
& & where $m_i = (p_i + q_i) / 2$\\
\hline
\end{tabular}
\caption{Similarity metrics.  JS is the Jensen-Shannon divergence and
  KL2 is a symmetric implementation of Kullback-Leibler divergence.}
\label{tab:metrics}
\end{table}

To judge the merit of each distance metric we obtained supervised
baseline scores using leave-one-out cross validation and the weighted
k-nearest-neighbor algorithm\footnote{Neighbors were weighted using
  1/distance, $k=30$ was chosen empirically.} on the gold tags of the
1M word WSJ corpus.  The results are listed in
Table~\ref{tab:distscores} sorted by score.

\begin{table}[ht] \centering
\begin{tabular}{|l|c|}
\hline
Metric & Accuracy(\%) \\
\hline
KL2 & ? \\
Manhattan & ? \\
Jensen & .7317 \\ %0.731718247078
Cosine & .7240 \\ %0.724018245545
Maximum & ? \\
Euclid & .6109 \\ %0.610962491672
lg2-Maximum & ? \\
lg2-Cosine & ? \\
lg2-Euclid & ? \\
lg2-Manhattan & ? \\
\hline
\end{tabular}
\caption{Supervised baseline scores with different distance metrics.
  Log-metric indicates that metric applied to the log of the
  probability vectors.}
\label{tab:distscores}
\end{table}

% K=1
% KL2 can we define kl2 on sparse vectors
% Manhattan 
% Jensen 0.684500999347
% Cosine 0.672916918704
% Maximum 
% Euclid 0.597443613122
% lg2-Maximum 
% lg2-Cosine 
% lg2-Euclid 
% lg2-Manhattan 

% K=20
% KL2 & ?\\
% Manhattan & ?\\
% Jensen & 0.732678404384\\
% Cosine & 0.720252797472\\
% Maximum & ?\\
% Euclid & 0.617086369856\\
% log-Maximum & ?\\
% log-Cosine & ?\\
% log-Euclid & ?\\
% log-Manhattan & ?\\

The entries with the log- prefix indicate a metric applied to the log
of the probability vectors.  Distance metrics on log probability
vectors performed poorly compared to their regular counterparts
indicating differences in low probability words are relatively
unimportant and high probability substitutes determine syntactic
category.  The surprisingly good result achieved by the simple Maximum
metric (which identifies the dimension with the largest difference
between two vectors) also support this conclusion.  The maximum score
of .73\% can be taken as a rough upper bound for an unsupervised
learner using this space on the 45-tag 1M word WSJ corpus because
.27\% of the instances are assigned to the wrong part of speech by the
majority of their closest neighbors.  We will discuss ways to push
this upper bound higher by including word type information together
with other features in Section~\ref{sec:scode} and
Section~\ref{sec:features}, respectively.

% moreover we are using probability vectors so its more natural to use
% kl2


