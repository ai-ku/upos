\section{Related Work}
\label{sec:related}

There are several good reviews of algorithms for unsupervised
part-of-speech induction
\cite{Christodoulopoulos:2010:TDU:1870658.1870714,Gao:2008:CBE:1613715.1613761}
and models of syntactic category acquisition \cite{ambridge2011child}.
In this review we focus on (i) the information captured by the inputs,
and (ii) the constraints applied to the outputs of the algorithms.

This work is to be distinguished from supervised part-of-speech
disambiguation systems, which use labeled training data
\cite{Church:1988:SPP:974235.974260}, unsupervised disambiguation
systems, which use a dictionary of possible tags for each word
\cite{Merialdo:1994:TET:972525.972526}, or prototype driven systems
which use a small set of prototypes for each class
\cite{Haghighi:2006:PLS:1220835.1220876}.  The problem of induction is
important for studying under-resourced languages that lack labeled
corpora and high quality dictionaries.  It is also essential in
modeling child language acquisition because every child manages to
induce syntactic categories without access to labeled sentences,
labeled prototypes, or dictionary constraints.

\paragraph{Clustering vs. HMMs:}
Models of unsupervised induction of part-of-speech categories fall
into two broad groups.  The first group uses algorithms that cluster
word types based on their context statistics
\cite{Schutze:1995:DPT:976973.976994}.  Work in modeling child
syntactic category acquisition has generally followed this clustering
approach \cite{redington1998distributional,mintz2003frequent}.  The
second group consists of probabilistic models based on the Hidden
Markov Model (HMM) framework \cite{Brown:1992:CNG:176313.176316}.

\paragraph{Clustering and data sparsity:}
Clustering based methods represent context using neighboring words,
typically a single word on the left and a single word on the right
called a ``frame'' (e.g., {\em {\bf the} dog {\bf is}; {\bf the} cat
  {\bf is}}).  They cluster word types rather than word tokens based
on the frames they occupy thus employing one-tag-per-word assumption
from the beginning (with the exception of some methods in
\cite{Schutze:1995:DPT:976973.976994}).  They may suffer from data
sparsity caused by infrequent words and infrequent contexts.  The
solutions suggested either restrict the set of words and set of
contexts to be clustered to the most frequently observed or use
dimensionality reduction.  \cite{redington1998distributional} defines
context similarity based on the number of common frames bypassing the
data sparsity problem but achieves mediocre results.
\cite{mintz2003frequent} only uses the most frequent 45 frames and
\cite{biemann2006unsupervised} clusters the most frequent 10,000 words
using contexts formed from the most frequent 150-200 words.
\cite{Schutze:1995:DPT:976973.976994,lamar-EtAl:2010:Short} employ SVD
to enhance similarity between less frequently observed words and
contexts.  \cite{Lamar:2010:LCU:1870658.1870736} represents each
context by the currently assigned left and right tag (which eliminates
data sparsity) and clusters word types using a soft k-means style
iterative algorithm.  They report the best clustering result to date
of 70.8\% many-to-one accuracy on a 45-tag 1M word corpus.

% all except schutze cluster word types

\paragraph{HMMs and distribution sparsity:}
The prototypical bitag HMM model maximizes the likelihood of the
corpus $w_1 \ldots w_n$ expressed as $P(w_1|c_1)\prod_{i=2}^n
P(w_i|c_i) P(c_i|c_{i-1})$ where $w_i$ are the word tokens and $c_i$
are their (hidden) tags.  One problem with such a model is its
tendency to distribute probabilities equally and the resulting
inability to model highly skewed word-tag distributions observed in
hand-labeled data \cite{johnson:2007:EMNLP-CoNLL2007}.  To favor
sparse word-tag distributions one can enforce a strict
one-tag-per-word solution
\cite{Brown:1992:CNG:176313.176316,Clark:2003:CDM:1067807.1067817},
use sparse priors in a Bayesian setting
\cite{goldwater-griffiths:2007:ACLMain,johnson:2007:EMNLP-CoNLL2007},
or use posterior regularization
\cite{Ganchev:2010:PRS:1859890.1859918}.  Each of these techniques
provide significant improvements over the standard HMM model: for
example \cite{Gao:2008:CBE:1613715.1613761} shows that sparse priors
can gain from 4\% (62\% to 66\% with a 1M word corpus) to 30\% (28\%
to 58\% with a 24K word corpus) in cross-validated many-to-one
accuracy.  However \cite{Christodoulopoulos:2010:TDU:1870658.1870714}
shows that the older one-tag-per-word models such as
\cite{Brown:1992:CNG:176313.176316} outperform the more sophisticated
sparse prior and posterior regularization methods both in speed and
accuracy (the Brown model gets 68\% many-to-one accuracy with a 1M
word corpus).  Given that close to 95\% of the word occurrences in
human labeled data are tagged with their most frequent part of speech
\cite{Lee:2010:STU:1870658.1870741}, this is probably not surprising;
one-tag-per-word is a fairly good first approximation for induction.

\paragraph{Poverty of features:}
Another problem with the plain HMM model is the poverty of its input
features.  Of the syntactic, semantic, and morphological information
linguists claim underlie syntactic categories, bitag HMMs only
represent limited syntactic information in their class based n-grams.
\cite{Clark:2003:CDM:1067807.1067817,bergkirkpatrick-klein:2010:ACL,blunsom-cohn:2011:ACL-HLT2011}
incorporate similar orthographic features and report improvements of
3, 7, and 10\% respectively over the baseline Brown model.
\cite{Christodoulopoulos:2010:TDU:1870658.1870714} use prototype based
features as described in \cite{Haghighi:2006:PLS:1220835.1220876} with
automatically induced prototypes and report an 8\% improvement over
the baseline Brown model.  Semantic information is incorporated to
some extent by some clustering based models and our paradigmatic
model.  To our knowledge, nobody has yet tried to incorporate
phonological or prosodic features in a computational model for
syntactic category acquisition.

\paragraph{Evaluation:}
It is natural to question the merit of evaluating unsupervised results
by comparing them to gold standard tags.  For example
\cite{freudenthal2005resolution} argue that a verb category (such as
{\sc vb} in Penn Treebank) that contains verbs that can and cannot be
used in certain constructions (e.g. the imperative) and verbs that can
be used as both auxiliaries and main verbs (e.g., {\em do, have}) does
not in fact constitute a set of items that could be substituted for
one another in particular sentences.  Such a category fails the
standard linguistic definition of a syntactic category and children do
not seem to make errors of substituting such words in utterances
(e.g. {\em''What do you want?''} vs. {\em *''What put you want?''}).
They suggest evaluating models by incorporating a production component
that allows the model's output to be compared to speech produced by
children exposed to the same input.  \cite{frank2009evaluating},
motivated by the lack of gold standards for many novel languages,
suggest comparing a system's clusters to a set of clusters created
from {\em substitutable frames}.  They create frames using two words
appearing in the corpus with exactly one word between and calculate
precision, recall, and F-score of the system's clustering.
Statistical parsers or factored machine learning systems could also be
sources of extrinsic evaluation for induced syntactic categories.  We
hope such extrinsic evaluation will be more widespread in the future
but nevertheless use the 45-tag Penn Treebank gold standard to
evaluate the current work.

\paragraph{Our work:}
Our work is more closely related to clustering based methods with two
important differences: (i) we represent word contexts not by an ad-hoc
selection of syntagmatic features based on a single word neighborhood
but by substitute vectors defined by a statistical language model
representing paradigmatic features of a larger window, (ii) we cluster
word tokens (or rather their contexts) instead of word types, which
means not being constrained by the one-tag-per-word assumption from
the beginning.  The features in our model incorporate semantic as well
as syntactic information from the context as illustrated by the
examples in Section~\ref{sec:lm}.  Our best results on the 45-tag 24K
corpus given in Section~\ref{sec:sparsity} (\collapseResult\% many-to-one)
compare favorably with best previously reported results on the same
corpus (58.2\% in \cite{Gao:2008:CBE:1613715.1613761}).  This
significant improvement is due to the additional information obtained
from the statistical language model which also comes at a great
computational cost.  Developing faster algorithms to find high
probability substitutes is in our current research agenda.

