\section{Substitute Vectors}
\label{sec:lm}

In this study, we predict the part of speech of a word in a given
context based on its substitute vector.  The dimensions of the
substitute vector represent words in the vocabulary, and the entries
in the substitute vector represent the probability of those words
being used in the given context substituting the target word.  This
section details the choice of the data set, the vocabulary and the
estimation of substitute probabilities.

% what is the test data
The first 24,020 tokens of the Penn Treebank \cite{treebank3} Wall
Street Journal Section 00 was used as the test corpus.
% what is the tag set
The treebank uses 45 part-of-speech tags which is the set we used as
the gold standard for comparison in our experiments.
% what is the LM training data
%Train => 5181717 126019973 690121813
To compute substitute probabilities we trained a language model using
approximately 126 million tokens of Wall Street Journal data
(1987-1994) extracted from CSR-III Text \cite{csr3text} (we excluded
the test corpus).
% how is the language model trained
We used SRILM \cite{Stolcke2002} to build a 4-gram language model with
Kneser-Ney discounting.
% what is the vocabulary
Words that were observed less than 500 times in the LM training data
were replaced by \textsc{unk} tags, which gave us a vocabulary size of
12,672.  
% perplexity
The perplexity of the 4-gram language model on the test corpus was
55.4 which is quite low due to using in-domain data and a small
vocabulary.

% how are the substitutes computed
We used both left and right neigbors to estimate the probabilities for
potential substitutes of word $w$ in a context $c_w$.  We define $c_w$
as the $2n-1$ word window centered around the target word position:
$w_{-n+1} \ldots w_0 \ldots w_{n-1}$ ($n=4$ is the LM order).  The
probability of a substitute word in a given context can be estimated
as:
\begin{eqnarray}
  \label{eq:lm1}P(w_0 = w | c_w) & \propto & P(w_{-n+1}\ldots w_0\ldots w_{n-1})\\
  \label{eq:lm2}& = & P(w_{-n+1})P(w_{-n+2}|w_{-n+1})\nonumber\\
  &&\ldots P(w_{n-1}|w_{-n+1}^{n-2})\\
  \label{eq:lm3}& \approx & P(w_0| w_{-n+1}^{-1})P(w_{1}|w_{-n+2}^0)\nonumber\\
  &&\ldots P(w_{n-1}|w_0^{n-2})
\end{eqnarray}
where $w_i^j$ represents the sequence of words $w_i w_{i+1} \ldots
w_{j}$.  In Equation \ref{eq:lm1}, $P(w|c_w)$ is proportional to
$P(w_{-n+1}\ldots w_0 \ldots w_{n+1})$ because the words of the
context are fixed.  Terms without $w_0$ are identical for each
substitute in Equation \ref{eq:lm2} therefore they have been dropped
in Equation \ref{eq:lm3}.  Finally, because of the Markov property of
n-gram language model, only the closest $n-1$ words are used in the
final conditional probability terms.

After computing the unnormalized probability of each of the 12,672
vocabulary words at each of the 24,020 positions in the test corpus
based on Equation~\ref{eq:lm3}, the probability vectors for each
position were normalized to add up to 1.0 giving us the final
substitute vectors used in the rest of this study.  Computation of
substitute vectors is computationally expensive and we are
investigating algorithmic methods that will provide the most likely
substitutes for each position without going through the whole
vocabulary.  However, the focus of this work is to first demonstrate
the usefulness of the substitute vector representation in learning
syntactic categories.

% example substitute vectors (both syntactic and semantic)
The high probability substitutes reflect both semantic and syntactic
features of the context as seen in the examples given in
Table~\ref{tab:subs}.  Top substitutes for the word ``the'' consist of
words that can act as determiners.  Top substitutes for ``board'' are
not only nouns, but specifically nouns compatible with the semantic
context.

\begin{table}[h]\centering
\begin{tabular}{|l|ll|} \hline
% \multicolumn{3}{|l|}
\emph{word} & \emph{substitute} & \emph{probability} \\ \hline
the	& its & .9011 \\
	& the & .0978 \\
	& a & .0007 \\ \hline
board	& company & .5573 \\
	& firm & .2219 \\
	& bank & .0727 \\ \hline
\end{tabular}
\caption{Example substitutes for ``the'' and ``board'' in the sentence {\em ``Pierre Vinken, 61 years old, will join the  board as a nonexecutive director Nov. 29.''}}
\label{tab:subs}
\end{table}

These examples illustrate two concerns inherent in all distributional
methods: (i) words that are generally substitutable like ``the'' and
``its'' are placed in separate categories ({\sc dt} and {\sc prp\$})
by the gold standard, (ii) words that are generally not substitutable
like ``board'' and ``banana'' are placed in the same category ({\sc
  nn}).  Whether gold standard part-of-speech tags or distributional
categories are better suited to applications like parsing or machine
translation can be best decided using extrinsic evaluation.  However
in this study we follow most previous work and evaluate our results by
comparing them to gold standard part-of-speech tags.
